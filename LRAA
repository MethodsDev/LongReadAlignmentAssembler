#!/usr/bin/env python3
# encoding: utf-8

import sys, os, re
import pysam
import argparse
import subprocess
import json
from multiprocessing import Process
from collections import defaultdict

sys.path.insert(
    0, os.path.sep.join([os.path.dirname(os.path.realpath(__file__)), "pylib"])
)
from Splice_graph import Splice_graph
from LRAA import LRAA
from Transcript import Transcript, GTF_contig_to_transcripts
from Quantify import Quantify
from ResourceMonitor import ResourceMonitor
import TranscriptFiltering
import Util_funcs
import LRAA_Globals
import MultiProcessManager as mpm
import logging
import shutil


VERSION = "v0.9.0-postdev"


# FORMAT = "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s at %(lineno)d :\n\t%(message)s\n"
FORMAT = (
    "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s:\n\t%(message)s\n"
)

logger = logging.getLogger()
logging.basicConfig(format=FORMAT, level=logging.INFO)


UTILDIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), "util")

num_threads = 4
CPU = num_threads
allow_spacers = False
single_best_only = False
prereconstruct_info_dir = "__prereconstruct"
INCLUDE_PRELIM_TRANSCRIPTS = False


def _contig_job_runner(job):
    # Module-scope function so it can be pickled by multiprocessing on macOS/Windows (spawn start method)
    contig_acc = job["contig_acc"]
    contig_strand = job["contig_strand"]
    genome_fasta = job["genome_fasta_filename"]
    # Propagate DEBUG flag into worker so debug-mode artifacts are produced consistently
    try:
        LRAA_Globals.DEBUG = bool(job.get("DEBUG", False))
    except Exception:
        pass
    # Propagate INCLUDE_PRELIM_TRANSCRIPTS into worker so prelim files are written without requiring --debug
    try:
        global INCLUDE_PRELIM_TRANSCRIPTS
        INCLUDE_PRELIM_TRANSCRIPTS = bool(job.get("INCLUDE_PRELIM_TRANSCRIPTS", False))
    except Exception:
        pass
    contig_seq_str = Util_funcs.retrieve_contig_seq_from_fasta_file(
        contig_acc, genome_fasta
    )

    # Ensure worker has config overrides and splice-graph params identical to parent
    if job.get("config_overrides"):
        # Shallow update into module-level config
        LRAA_Globals.config.update(job["config_overrides"])
    if job.get("splice_graph_params"):
        Splice_graph.init_sg_params(job["splice_graph_params"])

    input_transcripts = None
    if job["input_gtf"]:
        contig_to_input_transcripts = (
            GTF_contig_to_transcripts.parse_GTF_to_Transcripts(
                job["input_gtf"],
                contig_acc,
                contig_strand,
                job["restrict_region_lend"],
                job["restrict_region_rend"],
            )
        )
        input_transcripts = contig_to_input_transcripts.get(contig_acc, [])
        if input_transcripts is not None and len(input_transcripts) == 0:
            input_transcripts = None

    # Use a structured temp dir for contig/strand results
    tmp_root = f"{job['output_prefix']}.contigtmp"
    try:
        os.makedirs(tmp_root, exist_ok=True)
    except Exception:
        if not os.path.isdir(tmp_root):
            raise
    tmp_dir = os.path.join(tmp_root, f"{contig_acc}", f"{contig_strand}")
    os.makedirs(tmp_dir, exist_ok=True)
    tmp_prefix = os.path.join(tmp_dir, f"{contig_acc}.{contig_strand}")

    # optional per-worker resource monitor
    worker_monitor = None
    try:
        if LRAA_Globals.config.get("resource_monitor_enabled", False):
            tmp_root_m = f"{job['output_prefix']}.contigtmp"
            os.makedirs(tmp_root_m, exist_ok=True)
            tmp_dir_m = os.path.join(tmp_root_m, f"{contig_acc}", f"{contig_strand}")
            os.makedirs(tmp_dir_m, exist_ok=True)
            tmp_prefix_m = os.path.join(tmp_dir_m, f"{contig_acc}.{contig_strand}")
            monitor_path = f"{tmp_prefix_m}.resources.tsv"
            note = f"worker:{contig_acc}:{contig_strand}"
            worker_monitor = ResourceMonitor(
                monitor_path,
                interval_sec=float(LRAA_Globals.config.get("resource_monitor_interval", 2.0)),
                include_children=bool(
                    LRAA_Globals.config.get("resource_monitor_include_children", True)
                ),
                note=note,
            )
            worker_monitor.start()
    except Exception:
        pass

    # prepare writers
    quant_expr_tmp = f"{tmp_prefix}.quant.expr"
    quant_tracking_tmp = f"{tmp_prefix}.quant.tracking"

    # Headers
    quant_header = [
        "gene_id",
        "transcript_id",
        "uniq_reads",
        "all_reads",
        "isoform_fraction",
        "unique_gene_read_fraction",
        "TPM",
        "exons",
        "introns",
        "splice_hash_code",
    ]
    if not job["QUANT_ONLY"]:
        quant_header += ["splice_compat_contained", "splice_contained_by"]

    tracking_header = [
        "gene_id",
        "transcript_id",
        "transcript_splice_hash_code",
        "mp_id",
        "read_name",
        "frac_assigned",
    ]
    if job["DEBUG"]:
        tracking_header.append("read_weight")

    ofh_quant = open(quant_expr_tmp, "wt")
    print("\t".join(quant_header), file=ofh_quant)

    ofh_track = open(quant_tracking_tmp, "wt")
    print("\t".join(tracking_header), file=ofh_track)

    gtf_tmp = None
    ofh_gtf_local = None
    if not job["QUANT_ONLY"]:
        gtf_tmp = f"{tmp_prefix}.gtf"
        ofh_gtf_local = open(gtf_tmp, "wt")

    if job["QUANT_ONLY"]:
        run_quant_only(
            contig_acc,
            contig_strand,
            contig_seq_str,
            job["bam_file_for_sg"],
            job["bam_file_for_quant"],
            job["restrict_region_lend"],
            job["restrict_region_rend"],
            input_transcripts,
            ofh_quant,
            ofh_track,
            job["CPU_inner"],
            job["run_EM"],
            job["prereconstruct_info_dir"],
            report_quants=True,
        )
    else:
        run_transcript_assembly(
            contig_acc,
            contig_strand,
            contig_seq_str,
            job["bam_file_for_sg"],
            job["bam_file_for_quant"],
            job["restrict_region_lend"],
            job["restrict_region_rend"],
            input_transcripts,
            ofh_gtf_local,
            ofh_quant,
            ofh_track,
            job["CPU_inner"],
            job["run_EM"],
            job["single_best_only"],
            job["NO_FILTER_ISOFORMS"],
            job["min_isoform_fraction"],
            job["quant_EM_alpha"],
            job["prereconstruct_info_dir"],
            job["ME_only"],
            job["SE_only"],
        )

    ofh_quant.close()
    ofh_track.close()
    if ofh_gtf_local is not None:
        ofh_gtf_local.close()

    # write completion checkpoint so master can resume gracefully
    try:
        with open(f"{tmp_prefix}.ok", "wt") as ofh:
            print("ok", file=ofh)
    except Exception:
        pass

    if worker_monitor is not None:
        try:
            worker_monitor.stop()
        except Exception:
            pass

    return dict(
        contig_acc=contig_acc,
        contig_strand=contig_strand,
        quant_expr=quant_expr_tmp,
        quant_tracking=quant_tracking_tmp,
        gtf=gtf_tmp,
    )
def main():
    # CLI: define arguments and groups
    parser = argparse.ArgumentParser(
        description="LRAA: Long-read RNA isoform discovery and quantification",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Primary inputs and run mode
    primary = parser.add_argument_group("primary inputs")
    primary.add_argument("--bam", type=str, default=None, help="input BAM of aligned reads")
    primary.add_argument(
        "--bam_list",
        type=str,
        default=None,
        help="file listing cluster-specific BAMs (lines: '<cluster_id> <bam>' or '<bam>'); supported only with --quant_only; splice graph is built once from the union of all BAMs, then each cluster is quantified separately",
    )
    primary.add_argument("--genome", type=str, required=True, help="reference genome FASTA")
    primary.add_argument(
        "--gtf",
        type=str,
        default=None,
        help="reference transcripts GTF (required with --quant_only; optional to guide discovery)",
    )
    primary.add_argument(
        "--output_prefix",
        "-o",
        type=str,
        default="LRAA",
        help="output prefix for generated files",
    )
    primary.add_argument(
        "--quant_only",
        action="store_true",
        default=False,
        help="quantify only against provided --gtf (no isoform discovery)",
    )
    primary.add_argument(
        "--tag_bam",
        action="store_true",
        default=False,
        help="annotate BAM reads with assigned isoform info (writes a tagged BAM)",
    )
    primary.add_argument(
        "--no_norm", action="store_true", default=False, help="disable pre-assembly depth normalization"
    )
    primary.add_argument(
        "--normalize_max_cov_level",
        type=int,
        default=0,
        help="normalize BAM so per-base coverage is capped at this value prior to assembly (0 disables)",
    )
    primary.add_argument(
        "--single_best_only",
        action="store_true",
        default=False,
        help="during assembly, keep only the single best isoform per gene",
    )
    primary.add_argument("--CPU", type=int, default=num_threads, help="maximum CPU cores to use")
    primary.add_argument("--ME_only", action="store_true", default=False, help="restrict to multi-exon isoforms only")
    primary.add_argument("--SE_only", action="store_true", default=False, help="restrict to single-exon isoforms only")
    primary.add_argument(
        "--no_EM",
        action="store_true",
        default=False,
        help="do not run EM; divide ambiguous read support equally among assigned isoforms",
    )

    parser.add_argument(
        "--HiFi",
        action="store_true",
        default=False,
        help="high fidelity mode (more stringent): min_per_id: 98, infer_TSS, infer_PolyA, max_exon_spur_length: 13",
    )
    # Backward compatibility: accept deprecated --LowFi flag (now default); no-op with warning
    parser.add_argument(
        "--LowFi",
        action="store_true",
        default=False,
        help=argparse.SUPPRESS,
    )

    parser.add_argument(
        "--include_prelim_transcripts",
        action="store_true",
        default=False,
        help="write preliminary pre-filtered transcripts gtf and quant files",
    )

    parser.add_argument(
        "--version",
        action="store_true",
        default=False,
        help="display version: {}".format(VERSION),
    )

    parser.add_argument(
        "--parallelize_contigs",
        action="store_true",
        default=False,
        help="parallelize at (contig,strand) level; each job uses 1 core and outputs are merged at the end (respects --CPU as max concurrent jobs)",
    )


    parser.add_argument(
        "--no_resume_parallel",
        action="store_true",
        default=False,
        help="do not reuse existing per-(contig,strand) temp outputs when --parallelize_contigs; always recompute",
    )

    parser.add_argument(
        "--clean_parallel_tmp",
        action="store_true",
        default=False,
        help="after merging, remove per-(contig,strand) temp outputs (.quant.expr/.tracking/.gtf/.ok)",
    )

    ## debug params

    debug_group = parser.add_argument_group("debug settings")

    debug_group.add_argument(
        "--debug",
        "-d",
        action="store_true",
        default=False,
        help="debug mode, more verbose",
    )

    debug_group.add_argument(
        "--mpm_monitor", action="store_true", default=False
    )  # multiprocessing monitor

    debug_group.add_argument(
        "--no_filter_isoforms",
        action="store_true",
        default=False,
        help="do not filter any initially resolved isoforms",
    )

    ## config settings

    config_group = parser.add_argument_group("config settings")

    # disabling spacers for now - important for illumina or dirty long reads
    # config_group.add_argument("--allow_spacers", action='store_true', default=False)

    config_group.add_argument(
        "--num_total_reads",
        "-N",
        type=int,
        default=None,
        help="total number of reads for use in TPM calculations",
    )

    config_group.add_argument(
        "--min_path_score",
        type=float,
        default=LRAA_Globals.config["min_path_score"],
        help="minimum score for an isoform to be reported. default({})".format(
            LRAA_Globals.config["min_path_score"]
        ),
    )

    config_group.add_argument(
        "--min_per_id",
        type=float,
        default=LRAA_Globals.config["min_per_id"],
        help="min per_id for pacbio read alignments. default: ({})".format(
            LRAA_Globals.config["min_per_id"]
        ),
    )

    config_group.add_argument(
        "--max_intron_length",
        type=int,
        default=LRAA_Globals.config["max_intron_length"],
        help="maximum allowable intron length. default: ({})".format(
            LRAA_Globals.config["max_intron_length"]
        ),
    )

    config_group.add_argument(
        "--min_mapping_quality",
        type=int,
        default=LRAA_Globals.config["min_mapping_quality"],
        help="minimum read alignment mapping quality (default: {})".format(
            LRAA_Globals.config["min_mapping_quality"]
        ),
    )

    config_group.add_argument(
        "--min_isoform_fraction",
        type=float,
        default=LRAA_Globals.config["min_isoform_fraction"],
        help="exclude reconstructed isoforms that have read support less than this fraction of all isoforms at each gene (no impact on --quant_only). default: ({})".format(
            LRAA_Globals.config["min_isoform_fraction"]
        ),
    )

    config_group.add_argument(
        "--no_infer_TSS", action="store_true", default=False, help="do not infer TSS"
    )
    config_group.add_argument(
        "--no_infer_PolyA",
        action="store_true",
        default=False,
        help="do not infer_PolyA",
    )

    config_group.add_argument(
        "--min_monoexonic_TPM",
        type=float,
        required=False,
        default=LRAA_Globals.config["min_monoexonic_TPM"],
        help="minimum TPM for mono-exonic isoforms (default: {})".format(
            LRAA_Globals.config["min_monoexonic_TPM"]
        ),
    )

    config_group.add_argument(
        "--no_filter_internal_priming",
        action="store_true",
        default=False,
        help="do not filter isoforms that appear to derive from internal priming events",
    )

    config_group.add_argument(
        "--no_use_weighted_read_assignments",
        action="store_true",
        default=False,
        help="do not weight reads according to agreement with start/end of reads",
    )

    config_group.add_argument(
        "--ref_trans_filter_mode",
        default="retain_expressed",
        choices=["retain_expressed", "retain_filtered"],
        help="logic around retaining input reference transcripts in guided ID mode",
    )

    ## EM settings
    config_group_EM = parser.add_argument_group("EM settings")

    # config_group_EM.add_argument(
    #    "--EM_implementation",
    #    default="CGPT",
    #    choices=["CGPT", "BJH"],
    #    help="implementation of the EM alg to use.",
    # )

    config_group_EM.add_argument(
        "--EM_alpha",
        type=float,
        default=0.01,
        required=False,
        help="regularization factor EM_alpha * num_ambigous_reads for each transcript",
    )

    ## alt splice settings

    config_group_altsplice = parser.add_argument_group("alt splice settings")

    # TODO:// mv splice defaults to globals.config
    config_group_altsplice.add_argument(
        "--min_alt_splice_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_splice_freq"],
        help="min fraction required for alt splicing at an exon boundary (default: {})".format(
            LRAA_Globals.config["min_alt_splice_freq"]
        ),
    )

    config_group_altsplice.add_argument(
        "--min_alt_unspliced_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_unspliced_freq"],
        help="min fraction required for retained intron at splice boundary (default: {})".format(
            LRAA_Globals.config["min_alt_unspliced_freq"]
        ),
    )

    ## single cell settings
    config_group_single_cell = parser.add_argument_group("Single cell settings")

    config_group_single_cell.add_argument(
        "--cell_barcode_tag",
        type=str,
        default=LRAA_Globals.config["cell_barcode_tag"],
        help="bam tag for cell barcode",
    )

    config_group_single_cell.add_argument(
        "--read_umi_tag",
        type=str,
        default=LRAA_Globals.config["read_umi_tag"],
        help="bam tag for read umi",
    )

    ## restrict to contig and optionally region of contig

    contig_group_setting = parser.add_argument_group(
        "target specific contig (or region of contig)"
    )

    contig_group_setting.add_argument(
        "--contig", type=str, default=None, help="restrict run to single contig"
    )
    contig_group_setting.add_argument(
        "--region",
        type=str,
        default=None,
        help="restrict to region on contig chr\\d+[+-]?:\\d+-\\d+  ex. chr2:12345-56789 or chr2+:12345-56789 to restrict to the top strand",
    )

    ## generic config update via json
    parser.add_argument(
        "--config_update",
        type=str,
        required=False,
        help="JSON file containing key:value pairs to override entries in LRAA_Globals.config after other CLI options are processed",
    )

    # resource monitoring
    monitor_group = parser.add_argument_group("resource monitoring")
    monitor_group.add_argument(
        "--no_monitor_resources",
        action="store_true",
        default=False,
        help="disable resource usage monitoring (enabled by default); if enabled, writes <output_prefix>.resources.tsv and per-worker logs in parallel mode",
    )
    monitor_group.add_argument(
        "--monitor_interval",
        type=float,
        default=LRAA_Globals.config["resource_monitor_interval"],
        help="resource sampling interval in seconds (default: {})".format(
            LRAA_Globals.config["resource_monitor_interval"]
        ),
    )
    monitor_group.add_argument(
        "--monitor_children",
        action="store_true",
        default=LRAA_Globals.config["resource_monitor_include_children"],
        help="include child processes in aggregate resource metrics",
    )

    if "--version" in sys.argv:
        print("LRAA VERSION: {}".format(VERSION))
        if len(sys.argv) == 2:
            sys.exit(0)

    args = parser.parse_args()

    # require a BAM source
    if not args.bam and not args.bam_list:
        sys.exit("Error: Must specify --bam or --bam_list")

    if args.ME_only and args.SE_only:
        sys.exit("Error: Cannot specify both --ME_only and --SE_only")

    genome_fasta_filename = args.genome
    bam_filename = args.bam
    output_prefix = args.output_prefix
    # ensure global vars used by helper functions are updated
    global single_best_only, CPU, INCLUDE_PRELIM_TRANSCRIPTS
    single_best_only = args.single_best_only
    CPU = args.CPU
    QUANT_ONLY = args.quant_only
    TAG_BAM = args.tag_bam
    input_gtf = args.gtf
    NO_NORM = args.no_norm
    min_isoform_fraction = args.min_isoform_fraction
    run_EM = not args.no_EM
    ME_only = args.ME_only
    SE_only = args.SE_only

    NO_FILTER_ISOFORMS = args.no_filter_isoforms
    INCLUDE_PRELIM_TRANSCRIPTS = args.include_prelim_transcripts
    PARALLELIZE_CONTIGS = args.parallelize_contigs
    NO_RESUME_PARALLEL = args.no_resume_parallel
    CLEAN_PARALLEL_TMP = args.clean_parallel_tmp
    MONITOR_RESOURCES = not args.no_monitor_resources

    # if args.collapse:
    #    LRAA_Globals.config['collapse_alt_TSS_and_PolyA'] = True

    if args.quant_only and input_gtf is None:
        sys.exit(
            "If running --quant_only, must specify --gtf corresponding to targets of quantification"
        )

    # multi-cluster quant-only support
    MULTI_CLUSTER_BAMS = False
    cluster_bams = []  # list of dicts: {id, bam}
    if args.bam_list:
        if not args.quant_only:
            sys.exit("Error: --bam_list is supported only with --quant_only mode")
        if not os.path.exists(args.bam_list):
            sys.exit(f"Error, --bam_list file not found: {args.bam_list}")
        if args.num_total_reads is not None:
            sys.exit("Error: --num_total_reads cannot be used with --bam_list; per-cluster read counts are always derived from each BAM.")

        def _sanitize_cluster_id(s):
            # keep alnum, underscore, dash, dot
            return re.sub(r"[^A-Za-z0-9_.-]", "_", s)

        with open(args.bam_list, "rt") as fh:
            for raw in fh:
                line = raw.strip()
                if not line or line.startswith("#"):
                    continue
                parts = re.split(r"\s+", line)
                if len(parts) == 1:
                    bam_path = parts[0]
                    cluster_id = os.path.basename(bam_path)
                    cluster_id = re.sub(r"\.bam$", "", cluster_id)
                else:
                    cluster_id, bam_path = parts[0], parts[1]
                cluster_id = _sanitize_cluster_id(cluster_id)
                cluster_bams.append({"id": cluster_id, "bam": bam_path})
        if not cluster_bams:
            sys.exit("Error: --bam_list contains no BAM entries")
        MULTI_CLUSTER_BAMS = True

    ###############
    # update config
    LRAA_Globals.config["min_path_score"] = args.min_path_score
    LRAA_Globals.config["min_per_id"] = args.min_per_id
    LRAA_Globals.config["max_intron_length"] = args.max_intron_length
    LRAA_Globals.config["min_mapping_quality"] = args.min_mapping_quality
    LRAA_Globals.config["min_isoform_fraction"] = args.min_isoform_fraction
    # TSS/PolyA inference defaults are OFF for ONT/LowFi and are configured in the HiFi block below.
    # The --no_infer_* flags are only meaningful in HiFi mode and are handled there.
    LRAA_Globals.config["min_monoexonic_TPM"] = args.min_monoexonic_TPM
    LRAA_Globals.config["run_EM"] = run_EM
    LRAA_Globals.config["filter_internal_priming"] = not args.no_filter_internal_priming
    LRAA_Globals.config["use_weighted_read_assignments"] = (
        not args.no_use_weighted_read_assignments
    )
    # Deprecated: EM implementation selection removed; rely on current Quantify defaults
    LRAA_Globals.config["ref_trans_filter_mode"] = args.ref_trans_filter_mode

    quant_EM_alpha = args.EM_alpha
    LRAA_Globals.config["EM_alpha"] = quant_EM_alpha

    # resource monitor config
    LRAA_Globals.config["resource_monitor_enabled"] = MONITOR_RESOURCES
    LRAA_Globals.config["resource_monitor_interval"] = float(args.monitor_interval)
    LRAA_Globals.config["resource_monitor_include_children"] = bool(
        args.monitor_children
    )

    LRAA_Globals.config["cell_barcode_tag"] = args.cell_barcode_tag
    LRAA_Globals.config["read_umi_tag"] = args.read_umi_tag

    if args.LowFi:
        sys.exit(
            "Error: --LowFi has been removed. Defaults are now tuned for LowFi reads. Use --HiFi to enable the more stringent HiFi parameters."
        )

    if args.HiFi:
        # Override defaults with stringent HiFi settings
        # Enable TSS/PolyA by default in HiFi, but respect user-specified disabling flags
        LRAA_Globals.config["infer_TSS"] = False if args.no_infer_TSS else True
        LRAA_Globals.config["infer_PolyA"] = False if args.no_infer_PolyA else True
        LRAA_Globals.config["min_per_id"] = 98.0
        LRAA_Globals.config["max_exon_spur_length"] = 13
        LRAA_Globals.config["fracture_splice_graph_at_input_transcript_bounds"] = True
        LRAA_Globals.config["aggregate_adjacent_splice_boundaries"] = False
        # HiFi-specific splice frequency threshold
        LRAA_Globals.config["min_alt_splice_freq"] = 0.01
        # Ensure splice-graph params derive the HiFi value if user didn't explicitly override
        # If user did not pass --min_alt_splice_freq, argparse will have set it to the parser default,
        # which was computed before we overrode config. Force HiFi value unless user provided a value.
        default_min_alt = parser.get_default("min_alt_splice_freq")
        if args.min_alt_splice_freq == default_min_alt:
            args.min_alt_splice_freq = 0.01

    # apply generic config overrides last so they take precedence
    if args.config_update:
        cfg_file = args.config_update
        if not os.path.exists(cfg_file):
            sys.exit(f"Error, --config_update file not found: {cfg_file}")
        try:
            with open(cfg_file, "rt") as fh:
                user_cfg = json.load(fh)
        except Exception as e:
            sys.exit(f"Error parsing JSON config_update file {cfg_file}: {e}")

        if not isinstance(user_cfg, dict):
            sys.exit(
                f"Error, JSON in {cfg_file} must decode to an object with key:value pairs"
            )

        def _cast_value(orig_val, new_val):
            # attempt gentle casting to preserve expected primitive types
            try:
                if isinstance(orig_val, bool):
                    if isinstance(new_val, bool):
                        return new_val
                    if isinstance(new_val, (int, float)):
                        return bool(new_val)
                    if isinstance(new_val, str):
                        if new_val.lower() in ("true", "t", "1", "yes", "y"):
                            return True
                        if new_val.lower() in ("false", "f", "0", "no", "n"):
                            return False
                if isinstance(orig_val, int) and isinstance(new_val, (int, float, str)):
                    if isinstance(new_val, str):
                        if re.match(r"^-?\\d+$", new_val):
                            return int(new_val)
                        if re.match(r"^-?\\d+\\.0+$", new_val):
                            return int(float(new_val))
                    if isinstance(new_val, float) and new_val.is_integer():
                        return int(new_val)
                    if isinstance(new_val, int):
                        return new_val
                if isinstance(orig_val, float) and isinstance(
                    new_val, (int, float, str)
                ):
                    if isinstance(new_val, str) and re.match(
                        r"^-?\\d+(\\.\\d+)?$", new_val
                    ):
                        return float(new_val)
                    if isinstance(new_val, (int, float)):
                        return float(new_val)
            except Exception:
                pass
            # fallback raw
            return new_val

        applied_keys = []
        skipped_keys = []
        for k, v in user_cfg.items():
            if k not in LRAA_Globals.config:
                logger.warning(f"--config_update ignoring unknown config key: {k}")
                skipped_keys.append(k)
                continue
            orig = LRAA_Globals.config[k]
            newv = _cast_value(orig, v)
            LRAA_Globals.config[k] = newv
            applied_keys.append(k)
        logger.info(
            f"--config_update applied to {len(applied_keys)} keys: {', '.join(applied_keys) if applied_keys else 'None'}"
        )
        if skipped_keys:
            logger.info(f"--config_update skipped {len(skipped_keys)} unknown keys")

    if args.mpm_monitor:
        mpm.set_debug()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        LRAA_Globals.DEBUG = True

    allow_spacers = False
    # allow_spacers = args.allow_spacers

    if MULTI_CLUSTER_BAMS:
        # ensure each BAM has an index
        for cb in cluster_bams:
            bam_p = cb["bam"]
            if not os.path.exists(bam_p + ".bai"):
                logger.info(
                    f"-missing index for bam {bam_p}, will try to make it..."
                )
                subprocess.check_call("samtools index " + bam_p, shell=True)
    else:
        if not os.path.exists(bam_filename + ".bai"):
            logger.info(
                "-missing index for bam {}, will try to make it...".format(bam_filename)
            )
            subprocess.check_call("samtools index " + bam_filename, shell=True)

    ## perform normalization prior to assembly
    pre_norm_bam_filename = bam_filename

    bam_file_for_quant = pre_norm_bam_filename
    bam_file_for_sg = pre_norm_bam_filename

    if not MULTI_CLUSTER_BAMS:
        if args.num_total_reads is None:
            LRAA_Globals.config["num_total_reads"] = count_reads_from_bam(
                bam_file_for_quant
            )
            logger.info(
                "total number of reads counted from {} as: {}".format(
                    bam_file_for_quant, LRAA_Globals.config["num_total_reads"]
                )
            )

        else:
            LRAA_Globals.config["num_total_reads"] = args.num_total_reads
            logger.info("using total number of reads as: {}".format(args.num_total_reads))
            if args.num_total_reads == 0:
                raise RuntimeError("Error, number of total reads is set to zero.")

    if (not QUANT_ONLY) and (not NO_NORM) and (args.normalize_max_cov_level > 0):

        SS_bamsifter_prog = os.path.sep.join(
            [
                UTILDIR,
                "normalize_bam_by_strand.py",
            ]
        )

        norm_bam_filename = (
            os.path.basename(bam_filename) + f".norm_{args.normalize_max_cov_level}.bam"
        )

        cmd = " ".join(
            [
                SS_bamsifter_prog,
                f" --input_bam {bam_filename}",
                f" --normalize_max_cov_level {args.normalize_max_cov_level} ",
                f" --output_bam {norm_bam_filename} ",
            ]
        )

        norm_bam_checkpoint = norm_bam_filename + ".ok"
        if not os.path.exists(norm_bam_checkpoint):
            logger.info("generating normalized bam file")
            logger.info(cmd)
            subprocess.check_call(cmd, shell=True)

            subprocess.check_call(f"touch {norm_bam_checkpoint}", shell=True)

        bam_file_for_sg = norm_bam_filename

    ###########################################
    ## on to isoform discovery / reconstruction

    splice_graph_params_dict = {
        "read_aln_gap_merge_int": 10,
        "inter_exon_segment_merge_dist": 50,
        "max_genomic_contig_length": 1e10,
        "min_alt_splice_freq": args.min_alt_splice_freq,
        "min_alt_unspliced_freq": args.min_alt_unspliced_freq,
        "max_intron_length_for_exon_segment_filtering": 10000,
        "min_intron_support": 2,
        "min_terminal_splice_exon_anchor_length": 15,
        "remove_unspliced_introns": False,
    }
    # TODO:// put above settings into the config and add to cmd line interface

    Splice_graph.init_sg_params(splice_graph_params_dict)

    # data structures want:
    # ultimately exons and introns
    # build from reads.
    # define coverage intervals and introns as graph components.

    restricted_contig = None
    restrict_to_contig_orient = None

    if args.contig:
        restricted_contig = args.contig
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-])?$", restricted_contig)
        if m is not None:
            restricted_contig = m.group(1)
            restrict_to_contig_orient = m.group(2)

        genome_contigs_list = [restricted_contig]
    else:
        genome_contigs_list = get_genome_contigs_listing(genome_fasta_filename)

    restrict_region_lend, restrict_region_rend = None, None
    if args.region:
        args.region = args.region.replace(",", "")
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-])?:(\\d+)-(\\d+)$", args.region)
        if m is None:
            raise RuntimeError(
                "Error, could not parse region range: {}".format(args.region)
            )

        restricted_contig = m.group(1)

        genome_contigs_list = [restricted_contig]

        restrict_to_contig_orient = m.group(2)
        if restrict_to_contig_orient not in ("+", "-"):
            restrict_to_contig_orient = None

        restrict_region_lend = m.group(3)
        restrict_region_rend = m.group(4)

        restrict_region_lend = int(restrict_region_lend)
        restrict_region_rend = int(restrict_region_rend)
        assert (
            restrict_region_lend < restrict_region_rend
        ), f"Error, {args.region} invalid range"

        print(
            "{}\t{}\t{}\t{}".format(
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )

    if LRAA_Globals.DEBUG:
        if not os.path.exists(prereconstruct_info_dir):
            os.makedirs(prereconstruct_info_dir)

    contig_strand_to_input_transcripts = defaultdict(list)
    if input_gtf and not PARALLELIZE_CONTIGS:
        # Only preload transcripts in non-parallel mode; in contig-parallel mode, each worker parses GTF just for its (contig,strand)
        logger.info(f"-capturing input transcripts from gtf {input_gtf}")
        contig_to_input_transcripts = (
            GTF_contig_to_transcripts.parse_GTF_to_Transcripts(
                input_gtf,
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )
        for contig, transcript_obj_list in contig_to_input_transcripts.items():
            for transcript in transcript_obj_list:
                transcript_strand = transcript.get_strand()
                contig_strand_token = "{}^{}".format(contig, transcript_strand)
                contig_strand_to_input_transcripts[contig_strand_token].append(
                    transcript
                )

    ##----------------------------------------------
    ## Begin, target each contig separately

    # Special branch: multi-cluster quantification with shared splice graph evidence across all BAMs
    if MULTI_CLUSTER_BAMS and QUANT_ONLY:
        # build union of BAMs used for SG construction
        union_bams_for_sg = [cb["bam"] for cb in cluster_bams]

        # Pre-parse input transcripts in serial mode already done above

        # process each cluster independently
        for cb in cluster_bams:
            cluster_id = cb["id"]
            cluster_bam = cb["bam"]

            # set per-cluster total reads for TPM (always from BAM in bam_list mode)
            LRAA_Globals.config["num_total_reads"] = count_reads_from_bam(
                cluster_bam
            )
            logger.info(
                f"[{cluster_id}] total number of reads counted from {cluster_bam} as: {LRAA_Globals.config['num_total_reads']}"
            )

            output_prefix_cluster = f"{output_prefix}.{cluster_id}"

            # optional per-cluster main resource monitor
            main_monitor = None
            try:
                if LRAA_Globals.config.get("resource_monitor_enabled", False):
                    monitor_path_main = f"{output_prefix_cluster}.resources.tsv"
                    main_monitor = ResourceMonitor(
                        monitor_path_main,
                        interval_sec=LRAA_Globals.config["resource_monitor_interval"],
                        include_children=LRAA_Globals.config["resource_monitor_include_children"],
                        note=f"main:{cluster_id}",
                    )
                    main_monitor.start()
            except Exception:
                pass

            ofh_quant_output = None
            ofh_quant_read_tracker = None
            ofh_quant_output_filename = f"{output_prefix_cluster}.quant.expr"
            ofh_quant_read_tracking_filename = f"{output_prefix_cluster}.quant.tracking"

            if not PARALLELIZE_CONTIGS:
                ofh_quant_output = open(ofh_quant_output_filename, "wt")
                ofh_quant_read_tracker = open(ofh_quant_read_tracking_filename, "wt")
                # write headers
                tracking_file_header = [
                    "gene_id",
                    "transcript_id",
                    "transcript_splice_hash_code",
                    "mp_id",
                    "read_name",
                    "frac_assigned",
                ]
                if LRAA_Globals.DEBUG:
                    tracking_file_header.append("read_weight")
                print("\t".join(tracking_file_header), file=ofh_quant_read_tracker)

                header = [
                    "gene_id",
                    "transcript_id",
                    "uniq_reads",
                    "all_reads",
                    "isoform_fraction",
                    "unique_gene_read_fraction",
                    "TPM",
                    "exons",
                    "introns",
                    "splice_hash_code",
                ]
                # no splice-compat cols in quant-only
                print("\t".join(header), file=ofh_quant_output)

                for contig_acc in genome_contigs_list:
                    contig_seq_str = Util_funcs.retrieve_contig_seq_from_fasta_file(
                        contig_acc, genome_fasta_filename
                    )
                    for contig_strand in ("+", "-"):
                        if (
                            restrict_to_contig_orient is not None
                            and restrict_to_contig_orient != contig_strand
                        ):
                            continue

                        logger.info(
                            f"[{cluster_id}] -processing contig: {contig_acc} {contig_strand}"
                        )

                        contig_strand_token = f"{contig_acc}^{contig_strand}"
                        input_transcripts = contig_strand_to_input_transcripts[
                            contig_strand_token
                        ]
                        if input_transcripts is not None and len(input_transcripts) == 0:
                            input_transcripts = None

                        # optional per-(contig,strand) monitor in serial mode
                        contig_monitor = None
                        try:
                            if LRAA_Globals.config.get("resource_monitor_enabled", False):
                                tmp_root = f"{output_prefix_cluster}.contigtmp"
                                tmp_dir = os.path.join(tmp_root, f"{contig_acc}", f"{contig_strand}")
                                os.makedirs(tmp_dir, exist_ok=True)
                                contig_tmp_prefix = os.path.join(tmp_dir, f"{contig_acc}.{contig_strand}")
                                monitor_path = f"{contig_tmp_prefix}.resources.tsv"
                                note = f"serial:{cluster_id}:{contig_acc}:{contig_strand}"
                                contig_monitor = ResourceMonitor(
                                    monitor_path,
                                    interval_sec=float(LRAA_Globals.config.get("resource_monitor_interval", 2.0)),
                                    include_children=bool(
                                        LRAA_Globals.config.get("resource_monitor_include_children", True)
                                    ),
                                    note=note,
                                )
                                contig_monitor.start()
                        except Exception:
                            contig_monitor = None

                        run_quant_only(
                            contig_acc,
                            contig_strand,
                            contig_seq_str,
                            union_bams_for_sg,
                            cluster_bam,
                            restrict_region_lend,
                            restrict_region_rend,
                            input_transcripts,
                            ofh_quant_output,
                            ofh_quant_read_tracker,
                            CPU,
                            run_EM,
                            prereconstruct_info_dir,
                            report_quants=True,
                        )

                        if contig_monitor is not None:
                            try:
                                contig_monitor.stop()
                            except Exception:
                                pass

                ofh_quant_output.close()
                ofh_quant_read_tracker.close()

                # summarize per-contig monitors
                try:
                    if LRAA_Globals.config.get("resource_monitor_enabled", False):
                        processed = []
                        for contig_acc in genome_contigs_list:
                            for contig_strand in ("+", "-"):
                                if (
                                    restrict_to_contig_orient is not None
                                    and restrict_to_contig_orient != contig_strand
                                ):
                                    continue
                                processed.append((contig_acc, contig_strand))
                        _summarize_contig_resource_logs(output_prefix_cluster, processed, parallel_mode=False)
                except Exception:
                    pass

            else:
                # parallel contig mode per-cluster
                jobs = []
                for contig_acc in genome_contigs_list:
                    for contig_strand in ("+", "-"):
                        if (
                            restrict_to_contig_orient is not None
                            and restrict_to_contig_orient != contig_strand
                        ):
                            continue
                        jobs.append(
                            dict(
                                contig_acc=contig_acc,
                                contig_strand=contig_strand,
                                genome_fasta_filename=genome_fasta_filename,
                                bam_file_for_sg=union_bams_for_sg,
                                bam_file_for_quant=cluster_bam,
                                restrict_region_lend=restrict_region_lend,
                                restrict_region_rend=restrict_region_rend,
                                input_gtf=input_gtf,
                                config_overrides=dict(LRAA_Globals.config),
                                splice_graph_params=splice_graph_params_dict,
                                run_EM=run_EM,
                                single_best_only=single_best_only,
                                NO_FILTER_ISOFORMS=NO_FILTER_ISOFORMS,
                                min_isoform_fraction=min_isoform_fraction,
                                quant_EM_alpha=quant_EM_alpha,
                                prereconstruct_info_dir=prereconstruct_info_dir,
                                ME_only=ME_only,
                                SE_only=SE_only,
                                output_prefix=output_prefix_cluster,
                                QUANT_ONLY=True,
                                DEBUG=LRAA_Globals.DEBUG,
                                INCLUDE_PRELIM_TRANSCRIPTS=INCLUDE_PRELIM_TRANSCRIPTS,
                                skip=False,
                            )
                        )

                pool_size = max(1, min(CPU, len(jobs)))
                CPU_inner = max(1, CPU // pool_size)
                for j in jobs:
                    j["CPU_inner"] = CPU_inner

                if not NO_RESUME_PARALLEL:
                    resumed = 0
                    for j in jobs:
                        tmp_root = f"{j['output_prefix']}.contigtmp"
                        tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
                        tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
                        ok_path = f"{tmp_prefix}.ok"
                        have_expr = os.path.exists(f"{tmp_prefix}.quant.expr")
                        have_track = os.path.exists(f"{tmp_prefix}.quant.tracking")
                        have_gtf = True  # quant-only
                        if os.path.exists(ok_path) and have_expr and have_track and have_gtf:
                            j["skip"] = True
                            resumed += 1
                    if resumed:
                        logger.info(f"[{cluster_id}] Resuming {resumed} completed contig/strand jobs from previous run")

                mgr = mpm.MultiProcessManager(pool_size)
                for j in jobs:
                    if j.get("skip"):
                        continue
                    p = Process(target=_contig_job_runner, args=(j,))
                    mgr.launch_process(p)
                errs = mgr.wait_for_remaining_processes()
                if errs > 0:
                    raise RuntimeError(f"{errs} contig/strand jobs failed; see logs above")

                # merge per-cluster results
                merged_quant_fh = open(ofh_quant_output_filename, "wt")
                merged_track_fh = open(ofh_quant_read_tracking_filename, "wt")
                quant_header_final = [
                    "gene_id",
                    "transcript_id",
                    "uniq_reads",
                    "all_reads",
                    "isoform_fraction",
                    "unique_gene_read_fraction",
                    "TPM",
                    "exons",
                    "introns",
                    "splice_hash_code",
                ]
                print("\t".join(quant_header_final), file=merged_quant_fh)
                tracking_header_final = [
                    "gene_id",
                    "transcript_id",
                    "transcript_splice_hash_code",
                    "mp_id",
                    "read_name",
                    "frac_assigned",
                ]
                if LRAA_Globals.DEBUG:
                    tracking_header_final.append("read_weight")
                print("\t".join(tracking_header_final), file=merged_track_fh)

                def _append_without_first_line(src_path, dst_fh):
                    if src_path is None or (not os.path.exists(src_path)):
                        return
                    with open(src_path, "rt") as fh:
                        first = True
                        for line in fh:
                            if first:
                                first = False
                                continue
                            dst_fh.write(line)

                for j in jobs:
                    tmp_root = f"{j['output_prefix']}.contigtmp"
                    tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
                    tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
                    _append_without_first_line(f"{tmp_prefix}.quant.expr", merged_quant_fh)
                    _append_without_first_line(f"{tmp_prefix}.quant.tracking", merged_track_fh)

                merged_quant_fh.close()
                merged_track_fh.close()

                try:
                    if LRAA_Globals.config.get("resource_monitor_enabled", False):
                        _summarize_contig_resource_logs(output_prefix_cluster, jobs, parallel_mode=True)
                except Exception:
                    pass

                if CLEAN_PARALLEL_TMP:
                    for j in jobs:
                        tmp_root = f"{j['output_prefix']}.contigtmp"
                        tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
                        tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
                        for pth in (
                            f"{tmp_prefix}.quant.expr",
                            f"{tmp_prefix}.quant.tracking",
                            f"{tmp_prefix}.ok",
                            f"{tmp_prefix}.resources.tsv",
                        ):
                            if os.path.exists(pth):
                                try:
                                    os.remove(pth)
                                except Exception:
                                    pass

            # stop per-cluster main monitor
            if main_monitor is not None:
                try:
                    main_monitor.stop()
                except Exception:
                    pass

            # tag bam per cluster if requested
            if TAG_BAM:
                cmd = " ".join(
                    [
                        os.path.join(UTILDIR, "annotate_bam_with_read_tracking_info.py"),
                        "--bam",
                        cluster_bam,
                        "--tracking",
                        ofh_quant_read_tracking_filename,
                    ]
                )
                logger.info(f"[{cluster_id}] tagging bam file with transcripts assigned to each read")
                subprocess.check_call(cmd, shell=True)

        return

    # In contig-parallel mode, each worker writes per-task outputs; main merges later.
    ofh_quant_output = None
    ofh_quant_read_tracker = None
    ofh_quant_output_filename = f"{output_prefix}.quant.expr"
    ofh_quant_read_tracking_filename = f"{output_prefix}.quant.tracking"
    gtf_output_filename = f"{output_prefix}.gtf"

    # Always use the parallel execution machinery; restrict pool size when needed.
    if LRAA_Globals.DEBUG or INCLUDE_PRELIM_TRANSCRIPTS:
        logger.warning(
            "Parallel execution with --debug or --include_prelim_transcripts can overwrite debug/prelim files; constraining to a single worker (no concurrent contig processing)."
        )

    # optional main resource monitor
    main_monitor = None
    try:
        if LRAA_Globals.config.get("resource_monitor_enabled", False):
            monitor_path_main = f"{output_prefix}.resources.tsv"
            main_monitor = ResourceMonitor(
                monitor_path_main,
                interval_sec=LRAA_Globals.config["resource_monitor_interval"],
                include_children=LRAA_Globals.config["resource_monitor_include_children"],
                note="main",
            )
            main_monitor.start()
    except Exception:
        pass

    # Build contig/strand jobs and use a unified parallel scheduler (pool size may be 1)
    jobs = []
    for contig_acc in genome_contigs_list:
        for contig_strand in ("+", "-"):
            if (
                restrict_to_contig_orient is not None
                and restrict_to_contig_orient != contig_strand
            ):
                continue
            jobs.append(
                dict(
                    contig_acc=contig_acc,
                    contig_strand=contig_strand,
                    genome_fasta_filename=genome_fasta_filename,
                    bam_file_for_sg=bam_file_for_sg,
                    bam_file_for_quant=bam_file_for_quant,
                    restrict_region_lend=restrict_region_lend,
                    restrict_region_rend=restrict_region_rend,
                    input_gtf=input_gtf,
                    config_overrides=dict(LRAA_Globals.config),
                    splice_graph_params=splice_graph_params_dict,
                    run_EM=run_EM,
                    single_best_only=single_best_only,
                    NO_FILTER_ISOFORMS=NO_FILTER_ISOFORMS,
                    min_isoform_fraction=min_isoform_fraction,
                    quant_EM_alpha=quant_EM_alpha,
                    prereconstruct_info_dir=prereconstruct_info_dir,
                    ME_only=ME_only,
                    SE_only=SE_only,
                    output_prefix=output_prefix,
                    QUANT_ONLY=QUANT_ONLY,
                    DEBUG=LRAA_Globals.DEBUG,
                    INCLUDE_PRELIM_TRANSCRIPTS=INCLUDE_PRELIM_TRANSCRIPTS,
                    skip=False,
                )
            )

    # worker function declared at module scope (_contig_job_runner)

    # Determine pool size and cores per worker so total cores <= CPU
    requested_pool = max(1, min(CPU, len(jobs)))
    # Constrain to 1 worker if debug or prelim requested
    pool_size = 1 if (LRAA_Globals.DEBUG or INCLUDE_PRELIM_TRANSCRIPTS or not PARALLELIZE_CONTIGS) else requested_pool
    CPU_inner = max(1, CPU // pool_size)
    # set per-job CPU
    for j in jobs:
        j["CPU_inner"] = CPU_inner

    logger.info(
        f"Running {len(jobs)} contig/strand jobs with up to {pool_size} concurrent processes; CPU_inner={CPU_inner} (cap={CPU})"
    )

    # Launch in batches of size pool_size to avoid daemonic Pool workers (so inner component multiprocessing can spawn)
    # Before launching, mark jobs already completed (resume) if allowed
    if not NO_RESUME_PARALLEL:
        resumed = 0
        for j in jobs:
            tmp_root = f"{j['output_prefix']}.contigtmp"
            tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
            tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
            ok_path = f"{tmp_prefix}.ok"
            # require all expected outputs to be present
            have_expr = os.path.exists(f"{tmp_prefix}.quant.expr")
            have_track = os.path.exists(f"{tmp_prefix}.quant.tracking")
            have_gtf = True if j["QUANT_ONLY"] else os.path.exists(f"{tmp_prefix}.gtf")
            if os.path.exists(ok_path) and have_expr and have_track and have_gtf:
                j["skip"] = True
                resumed += 1
        if resumed:
            logger.info(f"Resuming {resumed} completed contig/strand jobs from previous run")

    # Use rolling scheduling to keep all slots busy
    logger.info("Using MultiProcessManager rolling scheduler for contig/strand jobs")
    mgr = mpm.MultiProcessManager(pool_size)
    for j in jobs:
        if j.get("skip"):
            continue
        p = Process(target=_contig_job_runner, args=(j,))
        mgr.launch_process(p)
    errs = mgr.wait_for_remaining_processes()
    if errs > 0:
        raise RuntimeError(f"{errs} contig/strand jobs failed; see logs above")

    # Merge results
    # Write merged headers
    merged_quant_fh = open(ofh_quant_output_filename, "wt")
    merged_track_fh = open(ofh_quant_read_tracking_filename, "wt")

    # Build and write headers deterministically (DEBUG off for merged header)
    quant_header_final = [
        "gene_id",
        "transcript_id",
        "uniq_reads",
        "all_reads",
        "isoform_fraction",
        "unique_gene_read_fraction",
        "TPM",
        "exons",
        "introns",
        "splice_hash_code",
    ]
    if not QUANT_ONLY:
        quant_header_final += ["splice_compat_contained", "splice_contained_by"]
    print("\t".join(quant_header_final), file=merged_quant_fh)

    tracking_header_final = [
        "gene_id",
        "transcript_id",
        "transcript_splice_hash_code",
        "mp_id",
        "read_name",
        "frac_assigned",
    ]
    if LRAA_Globals.DEBUG:
        tracking_header_final.append("read_weight")
    print("\t".join(tracking_header_final), file=merged_track_fh)

    # Append worker contents (skip first header line)
    def _append_without_first_line(src_path, dst_fh):
        if src_path is None or (not os.path.exists(src_path)):
            return
        with open(src_path, "rt") as fh:
            first = True
            for line in fh:
                if first:
                    first = False
                    continue
                dst_fh.write(line)

    for j in jobs:
        tmp_root = f"{j['output_prefix']}.contigtmp"
        tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
        tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
        _append_without_first_line(f"{tmp_prefix}.quant.expr", merged_quant_fh)
        _append_without_first_line(f"{tmp_prefix}.quant.tracking", merged_track_fh)

    merged_quant_fh.close()
    merged_track_fh.close()

    if not QUANT_ONLY:
        # Merge GTFs (no header lines in GTF)
        with open(gtf_output_filename, "wt") as gtf_out:
            for j in jobs:
                tmp_root = f"{j['output_prefix']}.contigtmp"
                tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
                tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
                gtf_tmp = f"{tmp_prefix}.gtf"
                if os.path.exists(gtf_tmp):
                    with open(gtf_tmp, "rt") as fh:
                        shutil.copyfileobj(fh, gtf_out)

    # write optional consolidated resource summary across contig/strand workers
    try:
        if LRAA_Globals.config.get("resource_monitor_enabled", False):
            _summarize_contig_resource_logs(output_prefix, jobs, parallel_mode=True)
    except Exception:
        pass

    # cleanup temp files
    if CLEAN_PARALLEL_TMP:
        for j in jobs:
            tmp_root = f"{j['output_prefix']}.contigtmp"
            tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
            tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
            for pth in (
                f"{tmp_prefix}.quant.expr",
                f"{tmp_prefix}.quant.tracking",
                f"{tmp_prefix}.gtf",
                f"{tmp_prefix}.ok",
                f"{tmp_prefix}.resources.tsv",
            ):
                if os.path.exists(pth):
                    try:
                        os.remove(pth)
                    except Exception:
                        pass

    # stop main resource monitor prior to final output conversions
    if main_monitor is not None:
        try:
            main_monitor.stop()
        except Exception:
            pass

    # make bed file for convenience
    if not QUANT_ONLY:
        bed_output_file = f"{output_prefix}.bed"
        cmd = " ".join(
            [
                os.path.join(
                    UTILDIR,
                    "gtf_to_bed_format.pl",
                ),
                gtf_output_filename,
                ">",
                bed_output_file,
            ]
        )
        logger.info("-making bed output file: {}".format(bed_output_file))
        subprocess.check_call(cmd, shell=True)

        if INCLUDE_PRELIM_TRANSCRIPTS or LRAA_Globals.DEBUG:
            # write bed for the prelim isoforms too, if the prelim GTF exists
            prelim_gtf = "__PRE-Tx-Filtering.transcripts.gtf"
            bed_output_file = "__PRE-Tx-Filtering.transcripts.bed"
            if os.path.exists(prelim_gtf) and os.path.getsize(prelim_gtf) > 0:
                cmd = " ".join(
                    [
                        os.path.join(
                            UTILDIR,
                            "gtf_to_bed_format.pl",
                        ),
                        prelim_gtf,
                        ">",
                        bed_output_file,
                    ]
                )
                logger.info("-making bed output file: {}".format(bed_output_file))
                subprocess.check_call(cmd, shell=True)
            else:
                logger.warning(
                    f"Skipping prelim BED generation; missing or empty GTF: {prelim_gtf}"
                )

    ##############################################################################
    ## Incorporate read-to-isoform assignment in the aligned bam file (optionally)

    if TAG_BAM:
        cmd = " ".join(
            [
                os.path.join(UTILDIR, "annotate_bam_with_read_tracking_info.py"),
                "--bam",
                bam_file_for_quant,
                "--tracking",
                ofh_quant_read_tracking_filename,
            ]
        )
        logger.info("tagging bam file with transcripts assigned to each read")
        subprocess.check_call(cmd, shell=True)

    return
    

def _summarize_contig_resource_logs(output_prefix, jobs_or_contigs, parallel_mode=False):
    """Write a consolidated summary TSV across per-(contig,strand) resource logs if monitoring enabled.
    Writes: <output_prefix>.resources.summary.tsv
    """
    try:
        if not LRAA_Globals.config.get("resource_monitor_enabled", False):
            return
        summary_path = f"{output_prefix}.resources.summary.tsv"
        with open(summary_path, "wt") as out:
            out.write("\t".join(["contig", "strand", "samples", "duration_sec", "peak_rss_mb_total", "avg_rss_mb_total", "peak_cpu_percent_total", "avg_cpu_percent_total"]) + "\n")
            for item in jobs_or_contigs:
                if parallel_mode:
                    contig_acc = item['contig_acc']
                    contig_strand = item['contig_strand']
                else:
                    contig_acc, contig_strand = item
                tmp_root = f"{output_prefix}.contigtmp"
                tmp_dir = os.path.join(tmp_root, f"{contig_acc}", f"{contig_strand}")
                tmp_prefix = os.path.join(tmp_dir, f"{contig_acc}.{contig_strand}")
                log_path = f"{tmp_prefix}.resources.tsv"
                if not os.path.exists(log_path):
                    continue
                # local summarization (avoid importing the util script here)
                try:
                    with open(log_path, "rt") as fh:
                        header = next(fh).rstrip("\n").split("\t")
                        idx = {k: i for i, k in enumerate(header)}
                        peak_rss = 0.0
                        peak_cpu = 0.0
                        sum_rss = 0.0
                        sum_cpu = 0.0
                        n = 0
                        first_ts = None
                        last_ts = None
                        for line in fh:
                            parts = line.rstrip("\n").split("\t")
                            try:
                                ts = float(parts[idx.get("epoch_ts", 0)])
                                rss = float(parts[idx.get("rss_mb", 2)])
                                cpu = float(parts[idx.get("cpu_percent", 3)])
                                rss_ch = float(parts[idx.get("rss_mb_children", 4)])
                                cpu_ch = float(parts[idx.get("cpu_percent_children", 5)])
                            except Exception:
                                continue
                            total_rss = rss + rss_ch
                            total_cpu = cpu + cpu_ch
                            peak_rss = max(peak_rss, total_rss)
                            peak_cpu = max(peak_cpu, total_cpu)
                            sum_rss += total_rss
                            sum_cpu += total_cpu
                            n += 1
                            if first_ts is None:
                                first_ts = ts
                            last_ts = ts
                        if n:
                            avg_rss = sum_rss / n
                            avg_cpu = sum_cpu / n
                            duration = (last_ts - first_ts) if (first_ts is not None and last_ts is not None) else 0.0
                            out.write("\t".join([
                                contig_acc,
                                contig_strand,
                                str(n),
                                f"{duration:.1f}",
                                f"{peak_rss:.1f}",
                                f"{avg_rss:.1f}",
                                f"{peak_cpu:.1f}",
                                f"{avg_cpu:.1f}",
                            ]) + "\n")
                except Exception:
                    continue
    except Exception:
        # summarization failures should not break the pipeline
        pass


def run_quant_only(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    bam_file_for_quant,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
    ofh_quant_output,
    ofh_quant_read_tracker,
    CPU,
    run_EM,
    prereconstruct_info_dir,
    report_quants=True,
):

    # get path assignments for the input transcripts
    # get the path assignments for the reads.
    # compare read mappings, assign categories

    # if no transcripts for this contig/strand, nothing to quant on
    if input_transcripts is None:
        logger.info(f"-no isoforms to quant on {contig_acc} [{contig_strand}]")
        # continue
        return

    sg = Splice_graph()

    # for quant only, build sg only based on the input gtf and not the alignments in the bam
    sg.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        input_transcripts,
        quant_mode=True,
        restrict_splice_type=None,
    )

    if sg.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    lraa_obj = LRAA(sg, CPU)
    logger.info("\n//SECTION QUANT: Assigning input transcript paths in graph")
    lraa_obj.assign_transcripts_paths_in_graph(input_transcripts)

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            input_transcripts, "__input_transcripts_path_in_graph"
        )

    logger.info("\n//SECTION QUANT: Assigning reads to paths in graph.")
    mp_counter = lraa_obj._populate_read_multi_paths(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_quant,
        allow_spacers,
        restrict_splice_type=None,
    )

    q = Quantify(
        run_EM, LRAA_Globals.config["max_EM_iterations_quant_only"], quant_mode="final"
    )
    logger.info("\n//SECTION QUANT: Quantifying transcripts according to read support.")
    read_name_to_fractional_transcript_assignment = q.quantify(
        sg, input_transcripts, mp_counter
    )

    if LRAA_Globals.DEBUG:
        q.dump_mp_to_transcripts_to_file(
            "__QUANTIFY_mp_to_transcripts_and_reads.tsv",
            contig_acc,
            contig_strand,
        )

    if report_quants:
        q.report_quant_results(
            input_transcripts,
            read_name_to_fractional_transcript_assignment,
            ofh_quant_output,
            ofh_quant_read_tracker,
        )

    return q


def run_transcript_assembly(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    bam_file_for_quant,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
    ofh_gtf,
    ofh_quant_output,
    ofh_quant_read_tracker,
    CPU,
    run_EM,
    single_best_only,
    NO_FILTER_ISOFORMS,
    min_isoform_fraction,
    quant_EM_alpha,
    prereconstruct_info_dir,
    ME_only,
    SE_only,
):

    ##---------------------
    ## Assemble transcripts

    LRAA_Globals.config["EM_alpha"] = 0.0  # disable during initial asm step.

    LRAA_Globals.LRAA_MODE = "ID-init_norm_reads"

    ##
    ## Multi-exon read processing section
    ##

    ME_input_transcripts = None
    if input_transcripts is not None:
        # extract the ME ones.
        ME_input_transcripts = [x for x in input_transcripts if x.has_introns()]
        if len(ME_input_transcripts) == 0:
            ME_input_transcripts = None

    ME_transcripts = None
    if not SE_only:  # build ME unless explicitly restricted to SE
        ME_transcripts = build_ME_transcripts(
            contig_acc,
            contig_strand,
            contig_seq_str,
            bam_file_for_sg,
            restrict_region_lend,
            restrict_region_rend,
            ME_input_transcripts,
        )

    ##
    ## Single-exon read processing section
    ##

    SE_input_transcripts = None
    if input_transcripts is not None:
        # extract the ME ones.
        SE_input_transcripts = [x for x in input_transcripts if not x.has_introns()]
        if len(SE_input_transcripts) == 0:
            SE_input_transcripts = None

    SE_transcripts = None
    if not ME_only:  # build SE unless explicitly restricted to ME
        SE_transcripts = build_SE_transcripts(
            contig_acc,
            contig_strand,
            contig_seq_str,
            bam_file_for_sg,
            restrict_region_lend,
            restrict_region_rend,
            SE_input_transcripts,
            SE_read_encapsulation_mask=ME_transcripts,
        )

    ##############################
    # Integrate ME and SE isoforms
    all_transcripts = list()
    if ME_transcripts is not None:
        all_transcripts += ME_transcripts
    if SE_transcripts is not None:
        all_transcripts += SE_transcripts

    if len(all_transcripts) == 0:
        return

    ## Build final Splice Graph
    sg_all = Splice_graph()
    logger.info(f"\n// -building splice graph for {contig_acc}")
    transcripts_incl_in_graph = list(all_transcripts)
    if input_transcripts is not None:
        # include only those input transcripts consistent with ME/SE restrictions
        if ME_only:
            transcripts_incl_in_graph += [
                t for t in input_transcripts if t.has_introns()
            ]
        elif SE_only:
            transcripts_incl_in_graph += [
                t for t in input_transcripts if not t.has_introns()
            ]
        else:
            transcripts_incl_in_graph += input_transcripts

    sg_all.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        transcripts_incl_in_graph,
        quant_mode=False,
        restrict_splice_type=None,
    )

    lraa_all_obj = LRAA(sg_all, CPU)

    lraa_all_obj.assign_transcripts_paths_in_graph(
        transcripts_incl_in_graph
    )  # limit to all input transcripts

    # reassign gene and transcript component ids based on shared nodes.
    logger.info("reclustering combined ME and SE transcripts to genes")
    all_transcripts = Transcript.recluster_transcripts_to_genes(
        all_transcripts, contig_acc, contig_strand
    )

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            all_transcripts, "__ME_and_SE_combined_transcripts_path_in_graph"
        )

    ###########################################################
    # Do an initial quant
    # Now use the original bam for quant (not depth normalized)
    LRAA_Globals.LRAA_MODE = "ID-full_reads"

    logger.info(
        "\n//SECTION QUANT using all reads for assembled isoforms: Assigning reads to paths in graph."
    )

    mp_all_counter = lraa_all_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_quant,
        allow_spacers,
        transcripts_incl_in_graph,
        restrict_splice_type=None,
    )

    # Ensure subsequent filtering/quant steps use ONLY real read evidence (not injected reference transcript pseudo-reads)
    # Rationale: earlier build_multipath_graph() calls (with input_transcripts) inject a synthetic "reftranscript:<id>" read per
    # reference transcript to guarantee a path in the graph. These pseudo-reads artificially inflate initial read support counts
    # and can cause inconsistencies vs. the final quant (which rebuilds without reinjecting them). By pruning them here we enforce
    # consistent real-read-based support across all downstream filtering and quantification passes.
    try:
        if mp_all_counter is not None:
            before_prune = (
                sum(
                    [
                        m.get_multipath_and_count()[1]
                        for m in mp_all_counter.get_all_MultiPathCountPairs()
                    ]
                )
                if hasattr(mp_all_counter, "get_all_MultiPathCountPairs")
                else None
            )
            mp_all_counter.prune_ref_transcripts_as_evidence()
            after_prune = (
                sum(
                    [
                        m.get_multipath_and_count()[1]
                        for m in mp_all_counter.get_all_MultiPathCountPairs()
                    ]
                )
                if hasattr(mp_all_counter, "get_all_MultiPathCountPairs")
                else None
            )
            if before_prune is not None and after_prune is not None:
                logger.info(
                    f"Pruned reference transcript pseudo-read evidence: total mp read counts {before_prune} -> {after_prune}"
                )
            else:
                logger.info(
                    "Pruned reference transcript pseudo-read evidence from multipath counter"
                )
    except Exception as e:
        logger.warning(f"Failed pruning reference transcript pseudo-read evidence: {e}")

    q = Quantify(
        run_EM,
        LRAA_Globals.config["max_EM_iterations_during_asm"],
        quant_mode="draft",
    )
    frac_read_assignments = q.quantify(sg_all, all_transcripts, mp_all_counter)

    if INCLUDE_PRELIM_TRANSCRIPTS or LRAA_Globals.DEBUG:

        with open("__PRE-Tx-Filtering.transcripts.gtf", "at") as ofh:
            for transcript in all_transcripts:
                ofh.write(transcript.to_GTF_format(include_TPM=False) + "\n")

        with open("__PRE-Tx-Filtering.quant.expr", "at") as ofh_prefilter_quant_expr:
            with open(
                "__PRE-Tx-Filtering.quant.tracking", "at"
            ) as ofh_prefilter_quant_tracking:
                q.report_quant_results(
                    all_transcripts,
                    frac_read_assignments,
                    ofh_prefilter_quant_expr,
                    ofh_prefilter_quant_tracking,
                )

    ##########################################
    ## Filtering of isoforms to remove 'noise'
    #########################################

    transcripts = all_transcripts  # legacy varname

    if input_transcripts is not None:
        # build splice pattern hash set for spliced reference transcripts only
        ref_splice_hashes = set(
            [
                Util_funcs.get_hash_code(t.get_introns_string())
                for t in input_transcripts
                if t.has_introns()
            ]
        )
        LRAA.differentiate_known_vs_novel_isoforms(
            transcripts, reference_splice_hashes=ref_splice_hashes
        )

    if NO_FILTER_ISOFORMS:

        logger.info("NOT FILTERING ISOFORMS.")

    else:

        ##################################################################
        ## initial filter of novel isoforms with insufficient read support

        logger.info("\n// Filtering novel isoforms by min read support")
        logger.info("Before filtering, have {} transcripts".format(len(transcripts)))
        min_read_support_novel_isoforms = LRAA_Globals.config["min_reads_novel_isoform"]
        transcripts = TranscriptFiltering.filter_novel_isoforms_by_min_read_support(
            transcripts, min_read_support_novel_isoforms
        )
        logger.info("After filtering, have {} transcripts".format(len(transcripts)))

        if len(transcripts) < 1:
            logger.info(
                "-no transcripts on contig {} {} survived min novel isoform read threshold requirement".format(
                    contig_acc, contig_strand
                )
            )
            # continue
            return

        # rerun quant for post-filtering
        logger.info(
            "\n// -rerunning quant post filtering novel isoforms via read thresholds"
        )
        frac_read_assignments = q.quantify(sg_all, transcripts, mp_all_counter)

        #####################################
        # pruning likely degradation products

        logger.info("\n// -pruning likely degradation products")
        logger.info("Before pruning, have {} transcripts".format(len(transcripts)))

        transcripts = TranscriptFiltering.prune_likely_degradation_products(
            transcripts, sg_all, frac_read_assignments
        )
        logger.info("After pruning, have {} transcripts".format(len(transcripts)))

        if len(transcripts) < 1:
            logger.info(
                "-no transcripts on contig {} {} survived pruning degradation products".format(
                    contig_acc, contig_strand
                )
            )
            # continue
            return

        # rerun quant for post-filtering
        logger.info("\n// -rerunning quant post-filtering of degradation products")
        frac_read_assignments = q.quantify(sg_all, transcripts, mp_all_counter)

        #######################################################
        # Filter isoforms according to minimum isoform fraction

        if min_isoform_fraction > 0:
            logger.info(
                "\n// -filtering isoforms according to minimum isoform fraction"
            )
            logger.info(
                "Before filtering, have {} transcripts".format(len(transcripts))
            )
            transcripts = TranscriptFiltering.filter_isoforms_by_min_isoform_fraction(
                transcripts,
                min_isoform_fraction,
                run_EM,
                LRAA_Globals.config["max_EM_iterations_during_asm"],
            )
            logger.info("After filtering, have {} transcripts".format(len(transcripts)))

            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived isoform filtering".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

        ##################################################################
        # Filter monoexonic transcripts according to expression thresholds
        if LRAA_Globals.config["min_monoexonic_TPM"] > 0:
            logger.info("\n// -filtering monoexonic transcripts based on min TPM")
            logger.info(
                "Before filtering, have {} transcripts".format(len(transcripts))
            )
            transcripts = (
                TranscriptFiltering.filter_monoexonic_isoforms_by_TPM_threshold(
                    transcripts, LRAA_Globals.config["min_monoexonic_TPM"]
                )
            )
            logger.info("After filtering, have {} transcripts".format(len(transcripts)))
            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived monoexonic filtering strategy".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

        ####################################################################
        # Filter isoforms apparently derived from internal priming artifacts
        if LRAA_Globals.config["filter_internal_priming"]:
            logger.info("\n// -filtering out internal priming")
            logger.info(
                "Before filtering, have {} transcripts".format(len(transcripts))
            )
            transcripts = TranscriptFiltering.filter_internally_primed_transcripts(
                transcripts,
                contig_seq_str,
                contig_strand,
                input_transcripts,
                restrict_filter_to_monoexonic=LRAA_Globals.config[
                    "restrict_internal_priming_filter_to_monoexonic"
                ],
            )
            logger.info("After filtering, have {} transcripts".format(len(transcripts)))
            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived internal priming filtering".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            transcripts, "__output_transcripts_path_in_graph"
        )

    ###################
    ## Final quant step
    ###################

    debug_mode_setting = LRAA_Globals.DEBUG
    if debug_mode_setting is True:
        LRAA_Globals.DEBUG = False

    LRAA_Globals.config["EM_alpha"] = quant_EM_alpha  # restore for final quant step.

    logger.info("Running final quant for {} transcripts".format(len(transcripts)))

    q = run_quant_only(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        bam_file_for_quant,
        restrict_region_lend,
        restrict_region_rend,
        transcripts,
        ofh_quant_output,
        ofh_quant_read_tracker,
        CPU,
        run_EM,
        prereconstruct_info_dir,
        report_quants=False,
    )

    # reset
    LRAA_Globals.DEBUG = debug_mode_setting

    ## one last filtering based on min isoform fraction

    transcripts_kept, transcripts_removed = (
        remove_low_isoform_fraction_transcripts_final_attempt(
            transcripts, min_isoform_fraction
        )
    )

    if len(transcripts_removed) > 0:
        transcripts = transcripts_kept

    frac_read_assignments = q._estimate_isoform_read_support(
        transcripts
    )  ##//TODO: make frac read assignments a member of the quantifier obj. Shouldn't need to run it again here unless transcripts got filtered above.

    ###############################################
    # examine splice compatible isoform differences (note, not currently filtering here, just annotating feature)
    (
        transcript_splice_compatible_containments,
        transcript_splice_compatible_contained_by,
    ) = TranscriptFiltering.evaluate_splice_compatible_alt_isoforms(transcripts)

    q.report_quant_results(
        transcripts,
        frac_read_assignments,
        ofh_quant_output,
        ofh_quant_read_tracker,
        splice_compatible_containments=transcript_splice_compatible_containments,
        splice_compatible_contained_by=transcript_splice_compatible_contained_by,
    )

    #######################################
    ## Done filtering, report final results

    ## report transcripts in GTF format
    logger.info(
        "writing gtf output for {} [{}] containing {} transcripts".format(
            contig_acc, contig_strand, len(transcripts)
        )
    )

    for transcript in transcripts:
        ofh_gtf.write(transcript.to_GTF_format() + "\n")

    return


def remove_low_isoform_fraction_transcripts_final_attempt(
    transcripts, min_isoform_fraction
):

    transcripts_kept = list()
    transcripts_removed = list()

    for transcript in transcripts:
        if transcript.get_isoform_fraction() < min_isoform_fraction:
            transcripts_removed.append(transcript)
        else:
            transcripts_kept.append(transcript)

    return transcripts_kept, transcripts_removed


def get_genome_contigs_listing(genome_fasta_filename):

    fai_file = "{}.fai".format(genome_fasta_filename)
    if not os.path.exists(fai_file):
        subprocess.check_call(
            "samtools faidx {}".format(genome_fasta_filename), shell=True
        )

    contigs_list = list()

    with open(fai_file) as fh:
        for line in fh:
            vals = line.split("\t")
            contig_acc = vals[0]
            contigs_list.append(contig_acc)

    return contigs_list


def report_transcript_paths_in_graph(transcripts, filename):

    with open(filename, "at") as ofh:
        for transcript in transcripts:
            sp = transcript.get_simple_path()
            exon_segments = transcript.get_exon_segments()
            transcript_id = transcript.get_transcript_id()
            print("\t".join([transcript_id, str(exon_segments), str(sp)]), file=ofh)

    return


def count_reads_from_bam(bam_filename):

    read_count_file = os.path.basename(bam_filename) + ".count"
    if not os.path.exists(read_count_file):
        subprocess.check_call(
            f"samtools view -c {bam_filename} > {read_count_file}.tmp", shell=True
        )
        os.rename(f"{read_count_file}.tmp", read_count_file)

    with open(read_count_file, "rt") as fh:
        count = next(fh)
        count = count.rstrip()
        count = int(count)
        assert count > 0, "Error, no reads counted from bam file..."
        return count


def build_ME_transcripts(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
):

    sg_ME = Splice_graph()

    ## Build ME Splice Graph
    logger.info(f"\n// -building splice graph for {contig_acc}")
    sg_ME.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        input_transcripts,
        quant_mode=False,
        restrict_splice_type="ME",
    )

    if sg_ME.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg_ME.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    # Incorporate reference transcripts if provided
    lraa_ME_obj = LRAA(sg_ME, CPU)

    if input_transcripts:
        lraa_ME_obj.assign_transcripts_paths_in_graph(
            input_transcripts
        )  # limit to ME input transcripts
        if LRAA_Globals.DEBUG:
            report_transcript_paths_in_graph(
                input_transcripts, "__input_transcripts_path_in_graph"
            )

    # build graph path nodes based on reads (and input transcripts) paths through graph.
    logger.info(f"\n// -building multpath graph for {contig_acc} {contig_strand}")
    mp_ME_counter = lraa_ME_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        allow_spacers,
        input_transcripts,
        restrict_splice_type="ME",
    )

    # Define ME isoforms
    logger.info(f"\n// -begin reconstructing isoforms for {contig_acc}")
    ME_transcripts = lraa_ME_obj.reconstruct_isoforms(single_best_only)

    if len(ME_transcripts) == 0:
        logger.info(
            "no ME transcripts constructed on {} {}".format(contig_acc, contig_strand)
        )

        return None

    else:
        if LRAA_Globals.DEBUG:
            with open("__ME_isoforms.gtf", "at") as tmp_ofh:
                for transcript in ME_transcripts:
                    tmp_ofh.write(transcript.to_GTF_format() + "\n")

        return ME_transcripts


def build_SE_transcripts(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    restrict_region_lend,
    restrict_region_rend,
    transcripts_incl_in_graph,
    SE_read_encapsulation_mask,
):

    sg_SE = Splice_graph()

    ## Build SE Splice Graph

    logger.info(f"\n// -building splice graph for {contig_acc}")
    sg_SE.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        transcripts_incl_in_graph,
        quant_mode=False,
        restrict_splice_type="SE",
        SE_read_encapsulation_mask=SE_read_encapsulation_mask,
    )

    if sg_SE.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg_SE.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    # Incorporate reference transcripts if provided
    lraa_SE_obj = LRAA(sg_SE, CPU)

    if transcripts_incl_in_graph:
        lraa_SE_obj.assign_transcripts_paths_in_graph(transcripts_incl_in_graph)
        if LRAA_Globals.DEBUG:
            report_transcript_paths_in_graph(
                transcripts_incl_in_graph, "__SE_incl_transcripts_path_in_graph"
            )

    # build graph path nodes based on reads (and input transcripts) paths through graph.
    logger.info(f"\n// -building multpath graph for {contig_acc} {contig_strand}")
    mp_SE_counter = lraa_SE_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        allow_spacers,
        transcripts_incl_in_graph,
        restrict_splice_type="SE",
        SE_read_encapsulation_mask=SE_read_encapsulation_mask,
    )

    #####################
    # Define SE isoforms

    logger.info(f"\n// -begin reconstructing isoforms for {contig_acc}")
    SE_transcripts = lraa_SE_obj.reconstruct_isoforms(single_best_only)

    if len(SE_transcripts) == 0:
        logger.info(
            "no SE transcripts constructed on {} {}".format(contig_acc, contig_strand)
        )
        # continue
        return None

    else:
        if LRAA_Globals.DEBUG:
            with open("__SE_isoforms.gtf", "at") as tmp_ofh:
                for transcript in SE_transcripts:
                    tmp_ofh.write(transcript.to_GTF_format() + "\n")

        return SE_transcripts


if __name__ == "__main__":
    main()
