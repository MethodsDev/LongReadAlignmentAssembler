#!/usr/bin/env python3
# encoding: utf-8

import sys, os, re
import pysam
import argparse
import subprocess
from collections import defaultdict

sys.path.insert(
    0, os.path.sep.join([os.path.dirname(os.path.realpath(__file__)), "pylib"])
)
from Splice_graph import Splice_graph
from LRAA import LRAA
from Transcript import Transcript, GTF_contig_to_transcripts
from Quantify import Quantify
import TranscriptFiltering
import Util_funcs
import LRAA_Globals
import MultiProcessManager as mpm
import logging
import lmdb
import shutil


# FORMAT = "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s at %(lineno)d :\n\t%(message)s\n"
FORMAT = (
    "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s:\n\t%(message)s\n"
)

logger = logging.getLogger()
logging.basicConfig(format=FORMAT, level=logging.INFO)


# VERSION="0.2.1"
VERSION = "BLEEDING_EDGE-v0.2.7"


def main():

    parser = argparse.ArgumentParser(
        description="LRAA: Long Read Alignment Assembler",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument("--bam", type=str, required=True, help="target bam file")
    parser.add_argument("--genome", type=str, required=True, help="target genome file")

    parser.add_argument(
        "--gtf",
        type=str,
        required=False,
        help="GTF to incorporate as guide during reconstruction or for targeting quant-only",
    )

    parser.add_argument(
        "--output_prefix", type=str, default="LRAA", help="prefix for output filenames"
    )

    parser.add_argument(
        "--single_best_only",
        action="store_true",
        default=False,
        help="only report the single highest scoring isoform per component",
    )

    parser.add_argument(
        "--CPU", type=int, default=1, help="number of cores for multithreading"
    )

    parser.add_argument(
        "--no_norm",
        action="store_true",
        default=False,
        help="do not run read coverage normalization before isoform reconstruction (equivalent to --normalize_max_cov_level 0)",
    )

    parser.add_argument(
        "--normalize_max_cov_level",
        type=int,
        default=LRAA_Globals.config["normalize_max_cov_level"],
        help="normalize to max read coverage level before assembly (default: {})".format(
            LRAA_Globals.config["normalize_max_cov_level"]
        ),
    )

    parser.add_argument(
        "--quant_only",
        action="store_true",
        default=False,
        help="perform quantification only (must specify --gtf with targets for quant)",
    )

    parser.add_argument(
        "--tag_bam",
        action="store_true",
        default=False,
        help="if performing quant, also output a copy of the BAM with additional tags containing classification and quant",
    )

    parser.add_argument(
        "--no_EM",
        action="store_true",
        default=False,
        help="do not run EM alg instead simply divide uncertain mappings equally among assigned target isoforms",
    )

    parser.add_argument(
        "--include_asm_draft_quant",
        action="store_true",
        default=False,
        help="include the draft (not real... based on norm reads) quant values at assembly. Potentially useful for isoform fraction info only",
    )

    parser.add_argument(
        "--version",
        action="store_true",
        default=False,
        help="display version: {}".format(VERSION),
    )

    ## debug params

    debug_group = parser.add_argument_group("debug settings")

    debug_group.add_argument(
        "--debug",
        "-d",
        action="store_true",
        default=False,
        help="debug mode, more verbose",
    )

    debug_group.add_argument(
        "--mpm_monitor", action="store_true", default=False
    )  # multiprocessing monitor

    ## config settings

    config_group = parser.add_argument_group("config settings")

    # disabling spacers for now - important for illumina or dirty long reads
    # config_group.add_argument("--allow_spacers", action='store_true', default=False)

    config_group.add_argument(
        "--num_total_reads",
        "-N",
        type=int,
        default=None,
        help="total number of reads for use in TPM calculations",
    )

    config_group.add_argument(
        "--min_path_score",
        type=float,
        default=LRAA_Globals.config["min_path_score"],
        help="minimum score for an isoform to be reported. default({})".format(
            LRAA_Globals.config["min_path_score"]
        ),
    )

    config_group.add_argument(
        "--min_per_id",
        type=float,
        default=LRAA_Globals.config["min_per_id"],
        help="min per_id for pacbio read alignments. default: ({})".format(
            LRAA_Globals.config["min_per_id"]
        ),
    )

    config_group.add_argument(
        "--min_mapping_quality",
        type=int,
        default=LRAA_Globals.config["min_mapping_quality"],
        help="minimum read alignment mapping quality (default: {})".format(
            LRAA_Globals.config["min_mapping_quality"]
        ),
    )

    config_group.add_argument(
        "--min_isoform_fraction",
        type=float,
        default=LRAA_Globals.config["min_isoform_fraction"],
        help="exclude reconstructed isoforms that have read support less than this fraction of all isoforms at each gene (no impact on --quant_only). default: ({})".format(
            LRAA_Globals.config["min_isoform_fraction"]
        ),
    )

    config_group.add_argument(
        "--no_infer_TSS", action="store_true", default=False, help="do not infer TSS"
    )
    config_group.add_argument(
        "--no_infer_PolyA",
        action="store_true",
        default=False,
        help="do not infer_PolyA",
    )

    config_group.add_argument(
        "--min_monoexonic_TPM",
        type=float,
        required=False,
        default=LRAA_Globals.config["min_monoexonic_TPM"],
        help="minimum TPM for mono-exonic isoforms (default: {})".format(
            LRAA_Globals.config["min_monoexonic_TPM"]
        ),
    )

    ## alt splice settings

    config_group_altsplice = parser.add_argument_group("alt splice settings")

    # TODO:// mv splice defaults to globals.config
    config_group_altsplice.add_argument(
        "--min_alt_splice_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_splice_freq"],
        help="min fraction required for alt splicing at an exon boundary (default: {})".format(
            LRAA_Globals.config["min_alt_splice_freq"]
        ),
    )

    config_group_altsplice.add_argument(
        "--min_alt_unspliced_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_unspliced_freq"],
        help="min fraction required for retained intron at splice boundary (default: {})".format(
            LRAA_Globals.config["min_alt_unspliced_freq"]
        ),
    )

    ## restrict to contig and optionally region of contig

    contig_group_setting = parser.add_argument_group(
        "target specific contig (or region of contig)"
    )

    contig_group_setting.add_argument(
        "--contig", type=str, default=None, help="restrict run to single contig"
    )
    contig_group_setting.add_argument(
        "--region",
        type=str,
        default=None,
        help="restrict to region on contig chr\\d+[+-]?:\\d+-\\d+  ex. chr2:12345-56789 or chr2+:12345-56789 to restrict to the top strand",
    )

    if "--version" in sys.argv:
        print("LRAA VERSION: {}".format(VERSION))
        if len(sys.argv) == 2:
            sys.exit(0)

    args = parser.parse_args()

    genome_fasta_filename = args.genome
    bam_filename = args.bam
    output_prefix = args.output_prefix
    single_best_only = args.single_best_only
    CPU = args.CPU
    QUANT_ONLY = args.quant_only
    TAG_BAM = args.tag_bam
    input_gtf = args.gtf
    NO_NORM = args.no_norm
    min_isoform_fraction = args.min_isoform_fraction
    run_EM = not args.no_EM

    # if args.collapse:
    #    LRAA_Globals.config['collapse_alt_TSS_and_PolyA'] = True

    if args.quant_only and input_gtf is None:
        sys.exit(
            "If running --quant_only, must specify --gtf corresponding to targets of quantification"
        )

    ###############
    # update config
    LRAA_Globals.config["min_path_score"] = args.min_path_score
    LRAA_Globals.config["min_per_id"] = args.min_per_id
    LRAA_Globals.config["min_mapping_quality"] = args.min_mapping_quality
    LRAA_Globals.config["min_isoform_fraction"] = args.min_isoform_fraction
    LRAA_Globals.config["infer_TSS"] = not args.no_infer_TSS
    LRAA_Globals.config["infer_PolyA"] = not args.no_infer_PolyA
    LRAA_Globals.config["min_monoexonic_TPM"] = args.min_monoexonic_TPM
    LRAA_Globals.config["run_EM"] = run_EM

    if args.mpm_monitor:
        mpm.set_debug()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        LRAA_Globals.DEBUG = True

    allow_spacers = False
    # allow_spacers = args.allow_spacers

    if not os.path.exists(bam_filename + ".bai"):
        logger.info(
            "-missing index for bam {}, will try to make it...".format(bam_filename)
        )
        subprocess.check_call("samtools index " + bam_filename, shell=True)

    ## perform normalization prior to assembly
    pre_norm_bam_filename = bam_filename

    bam_file_for_quant = pre_norm_bam_filename
    bam_file_for_sg = pre_norm_bam_filename

    if args.num_total_reads is None:
        LRAA_Globals.config["num_total_reads"] = count_reads_from_bam(
            bam_file_for_quant
        )
        logger.info(
            "total number of reads counted from {} as: {}".format(
                bam_file_for_quant, LRAA_Globals.config["num_total_reads"]
            )
        )

    else:
        LRAA_Globals.config["num_total_reads"] = args.num_total_reads
        logger.info("using total number of reads as: {}".format(args.num_total_reads))

    if (not QUANT_ONLY) and (not NO_NORM) and (args.normalize_max_cov_level > 0):

        SS_bamsifter_prog = os.path.sep.join(
            [
                os.path.dirname(os.path.realpath(__file__)),
                "util/normalize_bam_by_strand.py",
            ]
        )

        norm_bam_filename = (
            os.path.basename(bam_filename) + f".norm_{args.normalize_max_cov_level}.bam"
        )

        cmd = " ".join(
            [
                SS_bamsifter_prog,
                f" --input_bam {bam_filename}",
                f" --normalize_max_cov_level {args.normalize_max_cov_level} ",
                f" --output_bam {norm_bam_filename} ",
            ]
        )

        norm_bam_checkpoint = norm_bam_filename + ".ok"
        if not os.path.exists(norm_bam_checkpoint):
            logger.info("generating normalized bam file")
            logger.info(cmd)
            subprocess.check_call(cmd, shell=True)

            subprocess.check_call(f"touch {norm_bam_checkpoint}", shell=True)

        bam_file_for_sg = norm_bam_filename

    ###########################################
    ## on to isoform discovery / reconstruction

    splice_graph_params_dict = {
        "read_aln_gap_merge_int": 10,
        "inter_exon_segment_merge_dist": 50,
        "max_genomic_contig_length": 1e10,
        "min_alt_splice_freq": args.min_alt_splice_freq,
        "min_alt_unspliced_freq": args.min_alt_unspliced_freq,
        "max_intron_length_for_exon_segment_filtering": 10000,
        "min_intron_support": 1,
        "min_terminal_splice_exon_anchor_length": 15,
        "remove_unspliced_introns": False,
    }
    # TODO:// put above settings into the config and add to cmd line interface

    Splice_graph.init_sg_params(splice_graph_params_dict)

    # data structures want:
    # ultimately exons and introns
    # build from reads.
    # define coverage intervals and introns as graph components.

    restricted_contig = None
    restrict_to_contig_orient = None

    if args.contig:
        restricted_contig = args.contig
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-]?)$", restricted_contig)
        if m is not None:
            restricted_contig = m.group(1)
            restrict_to_contig_orient = m.group(2)

        genome_contigs_list = [restricted_contig]
    else:
        genome_contigs_list = get_genome_contigs_listing(genome_fasta_filename)

    restrict_region_lend, restrict_region_rend = None, None
    if args.region:
        args.region = args.region.replace(",", "")
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-]?):(\\d+)-(\\d+)$", args.region)
        if m is None:
            raise RuntimeError(
                "Error, could not parse region range: {}".format(args.region)
            )

        restricted_contig = m.group(1)

        genome_contigs_list = [restricted_contig]

        restrict_to_contig_orient = m.group(2)
        if restrict_to_contig_orient not in ("+", "-"):
            restrict_to_contig_orient = None

        restrict_region_lend = m.group(3)
        restrict_region_rend = m.group(4)

        restrict_region_lend = int(restrict_region_lend)
        restrict_region_rend = int(restrict_region_rend)
        assert (
            restrict_region_lend < restrict_region_rend
        ), f"Error, {args.region} invalid range"

        print(
            "{}\t{}\t{}\t{}".format(
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )

    prereconstruct_info_dir = "__prereconstruct"
    if LRAA_Globals.DEBUG:
        if not os.path.exists(prereconstruct_info_dir):
            os.makedirs(prereconstruct_info_dir)

    contig_strand_to_input_transcripts = defaultdict(list)
    if input_gtf:
        logger.info(f"-capturing input transcripts from gtf {input_gtf}")
        contig_to_input_transcripts = (
            GTF_contig_to_transcripts.parse_GTF_to_Transcripts(
                input_gtf,
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )
        for contig, transcript_obj_list in contig_to_input_transcripts.items():
            for transcript in transcript_obj_list:
                transcript_strand = transcript.get_strand()
                contig_strand_token = "{}^{}".format(contig, transcript_strand)
                contig_strand_to_input_transcripts[contig_strand_token].append(
                    transcript
                )

    ##----------------------------------------------
    ## Begin, target each contig separately

    ofh_quant_read_tracker = None
    ofh_quant_read_tracking_lmdb = None
    if QUANT_ONLY:
        output_filename = f"{output_prefix}.quant.expr"
        ofh_quant_read_tracking_filename = f"{output_prefix}.quant.tracking"
        ofh_quant_read_tracker = open(ofh_quant_read_tracking_filename, "wt")
        # write header
        print(
            "\t".join(["gene_id", "transcript_id", "read_name", "frac_assigned"]),
            file=ofh_quant_read_tracker,
        )

        if TAG_BAM:
            ofh_quant_read_tracking_lmdb_filename = (
                f"{output_prefix}.quant.tracking.lmdb"
            )
            # since the LMDB doesn't get reset automatically, make sure there isn't old data leftover
            if os.path.exists(ofh_quant_read_tracking_lmdb_filename):
                shutil.rmtree(ofh_quant_read_tracking_lmdb_filename)
            ofh_quant_read_tracking_lmdb = lmdb.open(
                ofh_quant_read_tracking_lmdb_filename,
                map_size=int(1e11),
                metasync=False,
                sync=False,
            )
    else:
        output_filename = f"{output_prefix}.gtf"

    ofh = open(output_filename, "wt")
    if QUANT_ONLY:
        # write header
        print(
            "\t".join(
                [
                    "gene_id",
                    "transcript_id",
                    "uniq_reads",
                    "all_reads",
                    "isoform_fraction",
                    "unique_gene_read_fraction",
                    "TPM",
                ]
            ),
            file=ofh,
        )

    for contig_acc in genome_contigs_list:

        contig_seq_str = Util_funcs.retrieve_contig_seq_from_fasta_file(
            contig_acc, genome_fasta_filename
        )

        for contig_strand in ("+", "-"):

            if (
                restrict_to_contig_orient is not None
                and restrict_to_contig_orient != contig_strand
            ):
                continue

            logger.info(f"-processing contig: {contig_acc} {contig_strand}")

            ##-------------------
            ## build splice graph

            sg = Splice_graph()

            contig_strand_token = "{}^{}".format(contig_acc, contig_strand)
            input_transcripts = contig_strand_to_input_transcripts[contig_strand_token]
            logger.info(
                "Have {} transcripts on {}".format(
                    len(input_transcripts), contig_strand_token
                )
            )

            if input_transcripts is not None and len(input_transcripts) == 0:
                input_transcripts = None

            if QUANT_ONLY:

                # get path assignments for the input transcripts
                # get the path assignments for the reads.
                # compare read mappings, assign categories

                # if no transcripts for this contig/strand, nothing to quant on
                if input_transcripts is None:
                    logger.info(
                        f"-no isoforms to quant on {contig_acc} [{contig_strand}]"
                    )
                    continue

                # for quant only, build sg only based on the input gtf and not the alignments in the bam
                sg.build_splice_graph_for_contig(
                    contig_acc,
                    contig_strand,
                    contig_seq_str,
                    bam_file_for_sg,
                    restrict_region_lend,
                    restrict_region_rend,
                    input_transcripts,
                    QUANT_ONLY,
                )

                if sg.is_empty():
                    logger.info(
                        f"-no splice graph created for contig: {contig_acc}.... skipping."
                    )
                    continue

                if LRAA_Globals.DEBUG:
                    sg.write_intron_exon_splice_graph_bed_files(
                        "{}/__prereconstruct.{}.{}.pad1".format(
                            prereconstruct_info_dir, contig_acc, contig_strand
                        ),
                        pad=1,
                    )

                lraa_obj = LRAA(sg, CPU)
                logger.debug(
                    "\n//SECTION QUANT: Assigning input transcript paths in graph"
                )
                lraa_obj.assign_transcripts_paths_in_graph(input_transcripts)

                if LRAA_Globals.DEBUG:
                    report_transcript_paths_in_graph(
                        input_transcripts, "__input_transcripts_path_in_graph"
                    )

                logger.debug("\n//SECTION QUANT: Assigning reads to paths in graph.")
                mp_counter = lraa_obj._populate_read_multi_paths(
                    contig_acc,
                    contig_strand,
                    contig_seq_str,
                    bam_file_for_quant,
                    allow_spacers,
                )

                q = Quantify(
                    run_EM, LRAA_Globals.config["max_EM_iterations_quant_only"]
                )
                logger.debug(
                    "\n//SECTION QUANT: Quantifying transcripts according to read support."
                )
                read_name_to_fractional_transcript_assignment = q.quantify(
                    sg, input_transcripts, mp_counter
                )
                q.report_quant_results(
                    input_transcripts,
                    read_name_to_fractional_transcript_assignment,
                    ofh,
                    ofh_quant_read_tracker,
                    ofh_quant_read_tracking_lmdb,
                )

            else:
                ##---------------------
                ## Assemble transcripts

                ## Build Splice Graph
                logger.info(f"-building splice graph for {contig_acc}")
                sg.build_splice_graph_for_contig(
                    contig_acc,
                    contig_strand,
                    contig_seq_str,
                    bam_file_for_sg,
                    restrict_region_lend,
                    restrict_region_rend,
                    input_transcripts,
                    QUANT_ONLY,
                )

                if sg.is_empty():
                    logger.info(
                        f"-no splice graph created for contig: {contig_acc}.... skipping."
                    )
                    continue

                if LRAA_Globals.DEBUG:
                    sg.write_intron_exon_splice_graph_bed_files(
                        "{}/__prereconstruct.{}.{}.pad1".format(
                            prereconstruct_info_dir, contig_acc, contig_strand
                        ),
                        pad=1,
                    )

                # Incorporate reference transcripts if provided
                lraa_obj = LRAA(sg, CPU)

                if input_transcripts:
                    lraa_obj.assign_transcripts_paths_in_graph(input_transcripts)
                    if LRAA_Globals.DEBUG:
                        report_transcript_paths_in_graph(
                            input_transcripts, "__input_transcripts_path_in_graph"
                        )

                # build graph path nodes based on reads (and input transcripts) paths through graph.
                logger.info(
                    f"-building multpath graph for {contig_acc} {contig_strand}"
                )
                lraa_obj.build_multipath_graph(
                    contig_acc,
                    contig_strand,
                    contig_seq_str,
                    bam_file_for_sg,
                    allow_spacers,
                    input_transcripts,
                )

                # Define isoforms
                logger.info(f"-begin reconstructing isoforms for {contig_acc}")
                transcripts = lraa_obj.reconstruct_isoforms(single_best_only)

                if input_transcripts is not None:
                    lraa_obj.prune_ref_transcripts_as_evidence(transcripts)

                # Do an initial quant
                q = Quantify(
                    run_EM, LRAA_Globals.config["max_EM_iterations_during_asm"]
                )
                frac_read_assignments = q._estimate_isoform_read_support(transcripts)

                ##########################################
                ## Filtering of isoforms to remove 'noise'
                #########################################

                # pruning likely degradation products
                transcripts = TranscriptFiltering.prune_likely_degradation_products(
                    transcripts, sg, frac_read_assignments
                )
                # rerun quant for post-filtering
                frac_read_assignments = q._estimate_isoform_read_support(transcripts)

                # Filter isoforms according to minimum isoform fraction
                if min_isoform_fraction > 0:
                    transcripts = (
                        TranscriptFiltering.filter_isoforms_by_min_isoform_fraction(
                            transcripts,
                            min_isoform_fraction,
                            run_EM,
                            LRAA_Globals.config["max_EM_iterations_during_asm"],
                        )
                    )

                # Filter monoexonic transcripts according to expression thresholds
                if LRAA_Globals.config["min_monoexonic_TPM"] > 0:
                    transcripts = (
                        TranscriptFiltering.filter_monoexonic_isoforms_by_TPM_threshold(
                            transcripts, LRAA_Globals.config["min_monoexonic_TPM"]
                        )
                    )

                # Filter isoforms apparently derived from internal priming artifacts
                if LRAA_Globals.config["filter_internal_priming"]:
                    transcripts = (
                        TranscriptFiltering.filter_internally_primed_transcripts(
                            transcripts, contig_seq_str
                        )
                    )

                #######################################
                ## Done filtering, report final results

                ## report transcripts in GTF format
                logger.info(
                    "writing gtf output for {} [{}] containing {} transcripts".format(
                        contig_acc, contig_strand, len(transcripts)
                    )
                )

                if LRAA_Globals.DEBUG:
                    report_transcript_paths_in_graph(
                        transcripts, "__output_transcripts_path_in_graph"
                    )

                for transcript in transcripts:
                    ofh.write(transcript.to_GTF_format() + "\n")

                if LRAA_Globals.DEBUG or args.include_asm_draft_quant:
                    debug_quant_ofh = open("__quant_info.tsv", "at")
                    debug_quant_read_tracking_ofh = open(
                        "__quant_info.reads_tracked.tsv", "at"
                    )
                    q = Quantify(
                        run_EM, LRAA_Globals.config["max_EM_iterations_quant_only"]
                    )
                    frac_read_assignments = q._estimate_isoform_read_support(
                        transcripts
                    )
                    q.report_quant_results(
                        transcripts,
                        frac_read_assignments,
                        debug_quant_ofh,
                        debug_quant_read_tracking_ofh,
                    )
                    debug_quant_ofh.close()
                    debug_quant_read_tracking_ofh.close()

    ofh.close()

    if QUANT_ONLY:
        ofh_quant_read_tracker.close()

        ##############################################################################
        ## Incorporate read-to-isoform assignment in the aligned bam file (optionally)

        if TAG_BAM:
            # explicitly delete the writeable lmdb environment to close it since there is no .close()
            del ofh_quant_read_tracking_lmdb

            # reopen in read-only mode
            ofh_quant_read_tracking_lmdb = lmdb.open(
                ofh_quant_read_tracking_lmdb_filename, readonly=True, lock=False
            )
            output_bam_filename = f"{output_prefix}.tagged.bam"

            with ofh_quant_read_tracking_lmdb.begin(
                write=False
            ) as txn, pysam.AlignmentFile(
                bam_filename, "rb", threads=4
            ) as bam_in, pysam.AlignmentFile(
                output_bam_filename, "wb", template=bam_in, threads=4
            ) as bam_out:
                for read in bam_in.fetch(until_eof=True):
                    data = txn.get(read.query_name.encode("utf-8"))
                    if data:
                        row = data.decode("utf-8").split(",")
                        if read.has_tag("XG") or read.has_tag("XI"):
                            raise ValueError(
                                "Error, read already has XG or XI tag, so cannot set them without overwriting"
                            )

                        gene_ids = ",".join(row[i] for i in range(0, len(row), 3))
                        transcript_ids = ",".join(
                            row[i + 1] for i in range(0, len(row), 3)
                        )
                        frac_assigned = ",".join(
                            row[i + 2] for i in range(0, len(row), 3)
                        )

                        read.set_tag("XG", gene_ids, "Z")
                        read.set_tag("XI", transcript_ids, "Z")
                        read.set_tag("XF", frac_assigned, "Z")

                    bam_out.write(read)

            ofh_quant_read_tracking_lmdb.close()

    return


def get_genome_contigs_listing(genome_fasta_filename):

    fai_file = "{}.fai".format(genome_fasta_filename)
    if not os.path.exists(fai_file):
        subprocess.check_call(
            "samtools faidx {}".format(genome_fasta_filename), shell=True
        )

    contigs_list = list()

    with open(fai_file) as fh:
        for line in fh:
            vals = line.split("\t")
            contig_acc = vals[0]
            contigs_list.append(contig_acc)

    return contigs_list


def report_transcript_paths_in_graph(transcripts, filename):

    with open(filename, "at") as ofh:
        for transcript in transcripts:
            sp = transcript.get_simple_path()
            exon_segments = transcript.get_exon_segments()
            transcript_id = transcript.get_transcript_id()
            print("\t".join([transcript_id, str(exon_segments), str(sp)]), file=ofh)

    return


def count_reads_from_bam(bam_filename):

    read_count_file = os.path.basename(bam_filename) + ".count"
    if not os.path.exists(read_count_file):
        subprocess.check_call(
            f"samtools view -c {bam_filename} > {read_count_file}.tmp", shell=True
        )
        os.rename(f"{read_count_file}.tmp", read_count_file)

    with open(read_count_file, "rt") as fh:
        count = next(fh)
        count = count.rstrip()
        count = int(count)
        assert count > 0, "Error, no reads counted from bam file..."
        return count


if __name__ == "__main__":
    main()
