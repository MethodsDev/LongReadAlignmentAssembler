#!/usr/bin/env python3
# encoding: utf-8

import sys, os, re
import pysam
import argparse
import subprocess
import json
from multiprocessing import Process
from collections import defaultdict

sys.path.insert(
    0, os.path.sep.join([os.path.dirname(os.path.realpath(__file__)), "pylib"])
)
from Splice_graph import Splice_graph
from LRAA import LRAA
from Transcript import Transcript, GTF_contig_to_transcripts
from Quantify import Quantify
import TranscriptFiltering
import Util_funcs
import LRAA_Globals
import MultiProcessManager as mpm
import logging
import shutil


VERSION = "v0.9.0-postdev"


# FORMAT = "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s at %(lineno)d :\n\t%(message)s\n"
FORMAT = (
    "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s:\n\t%(message)s\n"
)

logger = logging.getLogger()
logging.basicConfig(format=FORMAT, level=logging.INFO)


UTILDIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), "util")

num_threads = 4
CPU = num_threads
allow_spacers = False
single_best_only = False
prereconstruct_info_dir = "__prereconstruct"
INCLUDE_PRELIM_TRANSCRIPTS = False


def _contig_job_runner(job):
    # Module-scope function so it can be pickled by multiprocessing on macOS/Windows (spawn start method)
    contig_acc = job["contig_acc"]
    contig_strand = job["contig_strand"]
    genome_fasta = job["genome_fasta_filename"]
    contig_seq_str = Util_funcs.retrieve_contig_seq_from_fasta_file(
        contig_acc, genome_fasta
    )

    # Ensure worker has config overrides and splice-graph params identical to parent
    if job.get("config_overrides"):
        # Shallow update into module-level config
        LRAA_Globals.config.update(job["config_overrides"])
    if job.get("splice_graph_params"):
        Splice_graph.init_sg_params(job["splice_graph_params"])

    input_transcripts = None
    if job["input_gtf"]:
        contig_to_input_transcripts = (
            GTF_contig_to_transcripts.parse_GTF_to_Transcripts(
                job["input_gtf"],
                contig_acc,
                contig_strand,
                job["restrict_region_lend"],
                job["restrict_region_rend"],
            )
        )
        input_transcripts = contig_to_input_transcripts.get(contig_acc, [])
        if input_transcripts is not None and len(input_transcripts) == 0:
            input_transcripts = None

    # Use a structured temp dir for contig/strand results
    tmp_root = f"{job['output_prefix']}.contigtmp"
    try:
        os.makedirs(tmp_root, exist_ok=True)
    except Exception:
        if not os.path.isdir(tmp_root):
            raise
    tmp_dir = os.path.join(tmp_root, f"{contig_acc}", f"{contig_strand}")
    os.makedirs(tmp_dir, exist_ok=True)
    tmp_prefix = os.path.join(tmp_dir, f"{contig_acc}.{contig_strand}")

    # prepare writers
    quant_expr_tmp = f"{tmp_prefix}.quant.expr"
    quant_tracking_tmp = f"{tmp_prefix}.quant.tracking"

    # Headers
    quant_header = [
        "gene_id",
        "transcript_id",
        "uniq_reads",
        "all_reads",
        "isoform_fraction",
        "unique_gene_read_fraction",
        "TPM",
        "exons",
        "introns",
        "splice_hash_code",
    ]
    if not job["QUANT_ONLY"]:
        quant_header += ["splice_compat_contained", "splice_contained_by"]

    tracking_header = [
        "gene_id",
        "transcript_id",
        "transcript_splice_hash_code",
        "mp_id",
        "read_name",
        "frac_assigned",
    ]
    if job["DEBUG"]:
        tracking_header.append("read_weight")

    ofh_quant = open(quant_expr_tmp, "wt")
    print("\t".join(quant_header), file=ofh_quant)

    ofh_track = open(quant_tracking_tmp, "wt")
    print("\t".join(tracking_header), file=ofh_track)

    gtf_tmp = None
    ofh_gtf_local = None
    if not job["QUANT_ONLY"]:
        gtf_tmp = f"{tmp_prefix}.gtf"
        ofh_gtf_local = open(gtf_tmp, "wt")

    if job["QUANT_ONLY"]:
        run_quant_only(
            contig_acc,
            contig_strand,
            contig_seq_str,
            job["bam_file_for_sg"],
            job["bam_file_for_quant"],
            job["restrict_region_lend"],
            job["restrict_region_rend"],
            input_transcripts,
            ofh_quant,
            ofh_track,
            job["CPU_inner"],
            job["run_EM"],
            job["prereconstruct_info_dir"],
            report_quants=True,
        )
    else:
        run_transcript_assembly(
            contig_acc,
            contig_strand,
            contig_seq_str,
            job["bam_file_for_sg"],
            job["bam_file_for_quant"],
            job["restrict_region_lend"],
            job["restrict_region_rend"],
            input_transcripts,
            ofh_gtf_local,
            ofh_quant,
            ofh_track,
            job["CPU_inner"],
            job["run_EM"],
            job["single_best_only"],
            job["NO_FILTER_ISOFORMS"],
            job["min_isoform_fraction"],
            job["quant_EM_alpha"],
            job["prereconstruct_info_dir"],
            job["ME_only"],
            job["SE_only"],
        )

    ofh_quant.close()
    ofh_track.close()
    if ofh_gtf_local is not None:
        ofh_gtf_local.close()

    # write completion checkpoint so master can resume gracefully
    try:
        with open(f"{tmp_prefix}.ok", "wt") as ofh:
            print("ok", file=ofh)
    except Exception:
        pass

    return dict(
        contig_acc=contig_acc,
        contig_strand=contig_strand,
        quant_expr=quant_expr_tmp,
        quant_tracking=quant_tracking_tmp,
        gtf=gtf_tmp,
    )


def main():

    parser = argparse.ArgumentParser(
        description="LRAA: Long Read Alignment Assembler",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument("--bam", type=str, required=True, help="target bam file")
    parser.add_argument("--genome", type=str, required=True, help="target genome file")

    parser.add_argument(
        "--gtf",
        type=str,
        required=False,
        help="GTF to incorporate as guide during reconstruction or for targeting quant-only",
    )

    parser.add_argument(
        "--output_prefix", type=str, default="LRAA", help="prefix for output filenames"
    )

    parser.add_argument(
        "--single_best_only",
        action="store_true",
        default=False,
        help="only report the single highest scoring isoform per component",
    )

    parser.add_argument(
        "--CPU",
        type=int,
        default=num_threads,
        help="number of cores for multithreading (default: {})".format(num_threads),
    )

    parser.add_argument(
        "--no_norm",
        action="store_true",
        default=False,
        help="do not run read coverage normalization before isoform reconstruction (equivalent to --normalize_max_cov_level 0)",
    )

    parser.add_argument(
        "--normalize_max_cov_level",
        type=int,
        default=LRAA_Globals.config["normalize_max_cov_level"],
        help="normalize to max read coverage level before assembly (default: {})".format(
            LRAA_Globals.config["normalize_max_cov_level"]
        ),
    )

    parser.add_argument(
        "--quant_only",
        action="store_true",
        default=False,
        help="perform quantification only (must specify --gtf with targets for quant)",
    )

    parser.add_argument(
        "--ME_only",
        action="store_true",
        default=False,
        help="restrict transcript reconstruction to multi-exonic (ME) transcripts only",
    )

    parser.add_argument(
        "--SE_only",
        action="store_true",
        default=False,
        help="restrict transcript reconstruction to single-exon (SE) transcripts only",
    )

    parser.add_argument(
        "--tag_bam",
        action="store_true",
        default=False,
        help="if performing quant, also output a copy of the BAM with additional tags containing classification and quant",
    )

    parser.add_argument(
        "--no_EM",
        action="store_true",
        default=False,
        help="do not run EM alg instead simply divide uncertain mappings equally among assigned target isoforms",
    )

    parser.add_argument(
        "--LowFi",
        action="store_true",
        default=False,
        help="low fidelity mode: min_per_id: 80, no_infer_TSS, no_infer_PolyA, max_exon_spur_length: 20",
    )

    parser.add_argument(
        "--include_prelim_transcripts",
        action="store_true",
        default=False,
        help="write preliminary pre-filtered transcripts gtf and quant files",
    )

    parser.add_argument(
        "--version",
        action="store_true",
        default=False,
        help="display version: {}".format(VERSION),
    )

    parser.add_argument(
        "--parallelize_contigs",
        action="store_true",
        default=False,
        help="parallelize at (contig,strand) level; each job uses 1 core and outputs are merged at the end (respects --CPU as max concurrent jobs)",
    )

    parser.add_argument(
        "--no_resume_parallel",
        action="store_true",
        default=False,
        help="do not reuse existing per-(contig,strand) temp outputs when --parallelize_contigs; always recompute",
    )

    parser.add_argument(
        "--clean_parallel_tmp",
        action="store_true",
        default=False,
        help="after merging, remove per-(contig,strand) temp outputs (.quant.expr/.tracking/.gtf/.ok)",
    )

    ## debug params

    debug_group = parser.add_argument_group("debug settings")

    debug_group.add_argument(
        "--debug",
        "-d",
        action="store_true",
        default=False,
        help="debug mode, more verbose",
    )

    debug_group.add_argument(
        "--mpm_monitor", action="store_true", default=False
    )  # multiprocessing monitor

    debug_group.add_argument(
        "--no_filter_isoforms",
        action="store_true",
        default=False,
        help="do not filter any initially resolved isoforms",
    )

    ## config settings

    config_group = parser.add_argument_group("config settings")

    # disabling spacers for now - important for illumina or dirty long reads
    # config_group.add_argument("--allow_spacers", action='store_true', default=False)

    config_group.add_argument(
        "--num_total_reads",
        "-N",
        type=int,
        default=None,
        help="total number of reads for use in TPM calculations",
    )

    config_group.add_argument(
        "--min_path_score",
        type=float,
        default=LRAA_Globals.config["min_path_score"],
        help="minimum score for an isoform to be reported. default({})".format(
            LRAA_Globals.config["min_path_score"]
        ),
    )

    config_group.add_argument(
        "--min_per_id",
        type=float,
        default=LRAA_Globals.config["min_per_id"],
        help="min per_id for pacbio read alignments. default: ({})".format(
            LRAA_Globals.config["min_per_id"]
        ),
    )

    config_group.add_argument(
        "--max_intron_length",
        type=int,
        default=LRAA_Globals.config["max_intron_length"],
        help="maximum allowable intron length. default: ({})".format(
            LRAA_Globals.config["max_intron_length"]
        ),
    )

    config_group.add_argument(
        "--min_mapping_quality",
        type=int,
        default=LRAA_Globals.config["min_mapping_quality"],
        help="minimum read alignment mapping quality (default: {})".format(
            LRAA_Globals.config["min_mapping_quality"]
        ),
    )

    config_group.add_argument(
        "--min_isoform_fraction",
        type=float,
        default=LRAA_Globals.config["min_isoform_fraction"],
        help="exclude reconstructed isoforms that have read support less than this fraction of all isoforms at each gene (no impact on --quant_only). default: ({})".format(
            LRAA_Globals.config["min_isoform_fraction"]
        ),
    )

    config_group.add_argument(
        "--no_infer_TSS", action="store_true", default=False, help="do not infer TSS"
    )
    config_group.add_argument(
        "--no_infer_PolyA",
        action="store_true",
        default=False,
        help="do not infer_PolyA",
    )

    config_group.add_argument(
        "--min_monoexonic_TPM",
        type=float,
        required=False,
        default=LRAA_Globals.config["min_monoexonic_TPM"],
        help="minimum TPM for mono-exonic isoforms (default: {})".format(
            LRAA_Globals.config["min_monoexonic_TPM"]
        ),
    )

    config_group.add_argument(
        "--no_filter_internal_priming",
        action="store_true",
        default=False,
        help="do not filter isoforms that appear to derive from internal priming events",
    )

    config_group.add_argument(
        "--no_use_weighted_read_assignments",
        action="store_true",
        default=False,
        help="do not weight reads according to agreement with start/end of reads",
    )

    config_group.add_argument(
        "--ref_trans_filter_mode",
        default="retain_expressed",
        choices=["retain_expressed", "retain_filtered"],
        help="logic around retaining input reference transcripts in guided ID mode",
    )

    ## EM settings
    config_group_EM = parser.add_argument_group("EM settings")

    # config_group_EM.add_argument(
    #    "--EM_implementation",
    #    default="CGPT",
    #    choices=["CGPT", "BJH"],
    #    help="implementation of the EM alg to use.",
    # )

    config_group_EM.add_argument(
        "--EM_alpha",
        type=float,
        default=0.01,
        required=False,
        help="regularization factor EM_alpha * num_ambigous_reads for each transcript",
    )

    ## alt splice settings

    config_group_altsplice = parser.add_argument_group("alt splice settings")

    # TODO:// mv splice defaults to globals.config
    config_group_altsplice.add_argument(
        "--min_alt_splice_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_splice_freq"],
        help="min fraction required for alt splicing at an exon boundary (default: {})".format(
            LRAA_Globals.config["min_alt_splice_freq"]
        ),
    )

    config_group_altsplice.add_argument(
        "--min_alt_unspliced_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_unspliced_freq"],
        help="min fraction required for retained intron at splice boundary (default: {})".format(
            LRAA_Globals.config["min_alt_unspliced_freq"]
        ),
    )

    ## single cell settings
    config_group_single_cell = parser.add_argument_group("Single cell settings")

    config_group_single_cell.add_argument(
        "--cell_barcode_tag",
        type=str,
        default=LRAA_Globals.config["cell_barcode_tag"],
        help="bam tag for cell barcode",
    )

    config_group_single_cell.add_argument(
        "--read_umi_tag",
        type=str,
        default=LRAA_Globals.config["read_umi_tag"],
        help="bam tag for read umi",
    )

    ## restrict to contig and optionally region of contig

    contig_group_setting = parser.add_argument_group(
        "target specific contig (or region of contig)"
    )

    contig_group_setting.add_argument(
        "--contig", type=str, default=None, help="restrict run to single contig"
    )
    contig_group_setting.add_argument(
        "--region",
        type=str,
        default=None,
        help="restrict to region on contig chr\\d+[+-]?:\\d+-\\d+  ex. chr2:12345-56789 or chr2+:12345-56789 to restrict to the top strand",
    )

    ## generic config update via json
    parser.add_argument(
        "--config_update",
        type=str,
        required=False,
        help="JSON file containing key:value pairs to override entries in LRAA_Globals.config after other CLI options are processed",
    )

    if "--version" in sys.argv:
        print("LRAA VERSION: {}".format(VERSION))
        if len(sys.argv) == 2:
            sys.exit(0)

    args = parser.parse_args()

    if args.ME_only and args.SE_only:
        sys.exit("Error: Cannot specify both --ME_only and --SE_only")

    genome_fasta_filename = args.genome
    bam_filename = args.bam
    output_prefix = args.output_prefix
    # ensure global vars used by helper functions are updated
    global single_best_only, CPU, INCLUDE_PRELIM_TRANSCRIPTS
    single_best_only = args.single_best_only
    CPU = args.CPU
    QUANT_ONLY = args.quant_only
    TAG_BAM = args.tag_bam
    input_gtf = args.gtf
    NO_NORM = args.no_norm
    min_isoform_fraction = args.min_isoform_fraction
    run_EM = not args.no_EM
    ME_only = args.ME_only
    SE_only = args.SE_only

    NO_FILTER_ISOFORMS = args.no_filter_isoforms
    INCLUDE_PRELIM_TRANSCRIPTS = args.include_prelim_transcripts
    PARALLELIZE_CONTIGS = args.parallelize_contigs
    NO_RESUME_PARALLEL = args.no_resume_parallel
    CLEAN_PARALLEL_TMP = args.clean_parallel_tmp

    # if args.collapse:
    #    LRAA_Globals.config['collapse_alt_TSS_and_PolyA'] = True

    if args.quant_only and input_gtf is None:
        sys.exit(
            "If running --quant_only, must specify --gtf corresponding to targets of quantification"
        )

    ###############
    # update config
    LRAA_Globals.config["min_path_score"] = args.min_path_score
    LRAA_Globals.config["min_per_id"] = args.min_per_id
    LRAA_Globals.config["max_intron_length"] = args.max_intron_length
    LRAA_Globals.config["min_mapping_quality"] = args.min_mapping_quality
    LRAA_Globals.config["min_isoform_fraction"] = args.min_isoform_fraction
    LRAA_Globals.config["infer_TSS"] = not args.no_infer_TSS
    LRAA_Globals.config["infer_PolyA"] = not args.no_infer_PolyA
    LRAA_Globals.config["min_monoexonic_TPM"] = args.min_monoexonic_TPM
    LRAA_Globals.config["run_EM"] = run_EM
    LRAA_Globals.config["filter_internal_priming"] = not args.no_filter_internal_priming
    LRAA_Globals.config["use_weighted_read_assignments"] = (
        not args.no_use_weighted_read_assignments
    )
    LRAA_Globals.config["EM_implementation_use"] = "CGPT"  # args.EM_implementation
    LRAA_Globals.config["ref_trans_filter_mode"] = args.ref_trans_filter_mode

    quant_EM_alpha = args.EM_alpha
    LRAA_Globals.config["EM_alpha"] = quant_EM_alpha

    LRAA_Globals.config["cell_barcode_tag"] = args.cell_barcode_tag
    LRAA_Globals.config["read_umi_tag"] = args.read_umi_tag

    if args.LowFi:
        LRAA_Globals.config["infer_TSS"] = False
        LRAA_Globals.config["infer_PolyA"] = False
        LRAA_Globals.config["min_per_id"] = 80.0
        LRAA_Globals.config["max_exon_spur_length"] = 20
        LRAA_Globals.config["fracture_splice_graph_at_input_transcript_bounds"] = False
        LRAA_Globals.config["aggregate_adjacent_splice_boundaries"] = True

    # apply generic config overrides last so they take precedence
    if args.config_update:
        cfg_file = args.config_update
        if not os.path.exists(cfg_file):
            sys.exit(f"Error, --config_update file not found: {cfg_file}")
        try:
            with open(cfg_file, "rt") as fh:
                user_cfg = json.load(fh)
        except Exception as e:
            sys.exit(f"Error parsing JSON config_update file {cfg_file}: {e}")

        if not isinstance(user_cfg, dict):
            sys.exit(
                f"Error, JSON in {cfg_file} must decode to an object with key:value pairs"
            )

        def _cast_value(orig_val, new_val):
            # attempt gentle casting to preserve expected primitive types
            try:
                if isinstance(orig_val, bool):
                    if isinstance(new_val, bool):
                        return new_val
                    if isinstance(new_val, (int, float)):
                        return bool(new_val)
                    if isinstance(new_val, str):
                        if new_val.lower() in ("true", "t", "1", "yes", "y"):
                            return True
                        if new_val.lower() in ("false", "f", "0", "no", "n"):
                            return False
                if isinstance(orig_val, int) and isinstance(new_val, (int, float, str)):
                    if isinstance(new_val, str):
                        if re.match(r"^-?\\d+$", new_val):
                            return int(new_val)
                        if re.match(r"^-?\\d+\\.0+$", new_val):
                            return int(float(new_val))
                    if isinstance(new_val, float) and new_val.is_integer():
                        return int(new_val)
                    if isinstance(new_val, int):
                        return new_val
                if isinstance(orig_val, float) and isinstance(
                    new_val, (int, float, str)
                ):
                    if isinstance(new_val, str) and re.match(
                        r"^-?\\d+(\\.\\d+)?$", new_val
                    ):
                        return float(new_val)
                    if isinstance(new_val, (int, float)):
                        return float(new_val)
            except Exception:
                pass
            # fallback raw
            return new_val

        applied_keys = []
        skipped_keys = []
        for k, v in user_cfg.items():
            if k not in LRAA_Globals.config:
                logger.warning(f"--config_update ignoring unknown config key: {k}")
                skipped_keys.append(k)
                continue
            orig = LRAA_Globals.config[k]
            newv = _cast_value(orig, v)
            LRAA_Globals.config[k] = newv
            applied_keys.append(k)
        logger.info(
            f"--config_update applied to {len(applied_keys)} keys: {', '.join(applied_keys) if applied_keys else 'None'}"
        )
        if skipped_keys:
            logger.info(f"--config_update skipped {len(skipped_keys)} unknown keys")

    if args.mpm_monitor:
        mpm.set_debug()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        LRAA_Globals.DEBUG = True

    allow_spacers = False
    # allow_spacers = args.allow_spacers

    if not os.path.exists(bam_filename + ".bai"):
        logger.info(
            "-missing index for bam {}, will try to make it...".format(bam_filename)
        )
        subprocess.check_call("samtools index " + bam_filename, shell=True)

    ## perform normalization prior to assembly
    pre_norm_bam_filename = bam_filename

    bam_file_for_quant = pre_norm_bam_filename
    bam_file_for_sg = pre_norm_bam_filename

    if args.num_total_reads is None:
        LRAA_Globals.config["num_total_reads"] = count_reads_from_bam(
            bam_file_for_quant
        )
        logger.info(
            "total number of reads counted from {} as: {}".format(
                bam_file_for_quant, LRAA_Globals.config["num_total_reads"]
            )
        )

    else:
        LRAA_Globals.config["num_total_reads"] = args.num_total_reads
        logger.info("using total number of reads as: {}".format(args.num_total_reads))
        if args.num_total_reads == 0:
            raise RuntimeError("Error, number of total reads is set to zero.")

    if (not QUANT_ONLY) and (not NO_NORM) and (args.normalize_max_cov_level > 0):

        SS_bamsifter_prog = os.path.sep.join(
            [
                UTILDIR,
                "normalize_bam_by_strand.py",
            ]
        )

        norm_bam_filename = (
            os.path.basename(bam_filename) + f".norm_{args.normalize_max_cov_level}.bam"
        )

        cmd = " ".join(
            [
                SS_bamsifter_prog,
                f" --input_bam {bam_filename}",
                f" --normalize_max_cov_level {args.normalize_max_cov_level} ",
                f" --output_bam {norm_bam_filename} ",
            ]
        )

        norm_bam_checkpoint = norm_bam_filename + ".ok"
        if not os.path.exists(norm_bam_checkpoint):
            logger.info("generating normalized bam file")
            logger.info(cmd)
            subprocess.check_call(cmd, shell=True)

            subprocess.check_call(f"touch {norm_bam_checkpoint}", shell=True)

        bam_file_for_sg = norm_bam_filename

    ###########################################
    ## on to isoform discovery / reconstruction

    splice_graph_params_dict = {
        "read_aln_gap_merge_int": 10,
        "inter_exon_segment_merge_dist": 50,
        "max_genomic_contig_length": 1e10,
        "min_alt_splice_freq": args.min_alt_splice_freq,
        "min_alt_unspliced_freq": args.min_alt_unspliced_freq,
        "max_intron_length_for_exon_segment_filtering": 10000,
        "min_intron_support": 2,
        "min_terminal_splice_exon_anchor_length": 15,
        "remove_unspliced_introns": False,
    }
    # TODO:// put above settings into the config and add to cmd line interface

    Splice_graph.init_sg_params(splice_graph_params_dict)

    # data structures want:
    # ultimately exons and introns
    # build from reads.
    # define coverage intervals and introns as graph components.

    restricted_contig = None
    restrict_to_contig_orient = None

    if args.contig:
        restricted_contig = args.contig
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-])?$", restricted_contig)
        if m is not None:
            restricted_contig = m.group(1)
            restrict_to_contig_orient = m.group(2)

        genome_contigs_list = [restricted_contig]
    else:
        genome_contigs_list = get_genome_contigs_listing(genome_fasta_filename)

    restrict_region_lend, restrict_region_rend = None, None
    if args.region:
        args.region = args.region.replace(",", "")
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-])?:(\\d+)-(\\d+)$", args.region)
        if m is None:
            raise RuntimeError(
                "Error, could not parse region range: {}".format(args.region)
            )

        restricted_contig = m.group(1)

        genome_contigs_list = [restricted_contig]

        restrict_to_contig_orient = m.group(2)
        if restrict_to_contig_orient not in ("+", "-"):
            restrict_to_contig_orient = None

        restrict_region_lend = m.group(3)
        restrict_region_rend = m.group(4)

        restrict_region_lend = int(restrict_region_lend)
        restrict_region_rend = int(restrict_region_rend)
        assert (
            restrict_region_lend < restrict_region_rend
        ), f"Error, {args.region} invalid range"

        print(
            "{}\t{}\t{}\t{}".format(
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )

    if LRAA_Globals.DEBUG:
        if not os.path.exists(prereconstruct_info_dir):
            os.makedirs(prereconstruct_info_dir)

    contig_strand_to_input_transcripts = defaultdict(list)
    if input_gtf and not PARALLELIZE_CONTIGS:
        # Only preload transcripts in non-parallel mode; in contig-parallel mode, each worker parses GTF just for its (contig,strand)
        logger.info(f"-capturing input transcripts from gtf {input_gtf}")
        contig_to_input_transcripts = (
            GTF_contig_to_transcripts.parse_GTF_to_Transcripts(
                input_gtf,
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )
        for contig, transcript_obj_list in contig_to_input_transcripts.items():
            for transcript in transcript_obj_list:
                transcript_strand = transcript.get_strand()
                contig_strand_token = "{}^{}".format(contig, transcript_strand)
                contig_strand_to_input_transcripts[contig_strand_token].append(
                    transcript
                )

    ##----------------------------------------------
    ## Begin, target each contig separately

    # In contig-parallel mode, each worker writes per-task outputs; main merges later.
    ofh_quant_output = None
    ofh_quant_read_tracker = None
    ofh_quant_output_filename = f"{output_prefix}.quant.expr"
    ofh_quant_read_tracking_filename = f"{output_prefix}.quant.tracking"
    gtf_output_filename = f"{output_prefix}.gtf"
    if not PARALLELIZE_CONTIGS:
        ofh_quant_output = open(ofh_quant_output_filename, "wt")
        ofh_quant_read_tracker = open(ofh_quant_read_tracking_filename, "wt")
        # write header for tracking
        tracking_file_header = [
            "gene_id",
            "transcript_id",
            "transcript_splice_hash_code",
            "mp_id",
            "read_name",
            "frac_assigned",
        ]
        if LRAA_Globals.DEBUG:
            tracking_file_header.append("read_weight")
        print("\t".join(tracking_file_header), file=ofh_quant_read_tracker)

        ofh_gtf = open(gtf_output_filename, "wt")

        # write header for quant expr
        header = [
            "gene_id",
            "transcript_id",
            "uniq_reads",
            "all_reads",
            "isoform_fraction",
            "unique_gene_read_fraction",
            "TPM",
            "exons",
            "introns",
            "splice_hash_code",
        ]
        if not QUANT_ONLY:
            header += ["splice_compat_contained", "splice_contained_by"]
        print("\t".join(header), file=ofh_quant_output)

    if PARALLELIZE_CONTIGS:
        if LRAA_Globals.DEBUG or INCLUDE_PRELIM_TRANSCRIPTS:
            logger.warning(
                "--parallelize_contigs is incompatible with --debug or --include_prelim_transcripts; proceeding may overwrite debug/prelim files. Disabling parallelization for safety."
            )
            PARALLELIZE_CONTIGS = False

    if not PARALLELIZE_CONTIGS:
        for contig_acc in genome_contigs_list:

            contig_seq_str = Util_funcs.retrieve_contig_seq_from_fasta_file(
                contig_acc, genome_fasta_filename
            )

            for contig_strand in ("+", "-"):

                if (
                    restrict_to_contig_orient is not None
                    and restrict_to_contig_orient != contig_strand
                ):
                    continue

                logger.info(f"-processing contig: {contig_acc} {contig_strand}")

                contig_strand_token = "{}^{}".format(contig_acc, contig_strand)
                input_transcripts = contig_strand_to_input_transcripts[
                    contig_strand_token
                ]
                logger.info(
                    "Have {} transcripts on {}".format(
                        len(input_transcripts), contig_strand_token
                    )
                )

                if input_transcripts is not None and len(input_transcripts) == 0:
                    input_transcripts = None

                if QUANT_ONLY:
                    run_quant_only(
                        contig_acc,
                        contig_strand,
                        contig_seq_str,
                        bam_file_for_sg,
                        bam_file_for_quant,
                        restrict_region_lend,
                        restrict_region_rend,
                        input_transcripts,
                        ofh_quant_output,
                        ofh_quant_read_tracker,
                        CPU,
                        run_EM,
                        prereconstruct_info_dir,
                        report_quants=True,
                    )

                else:
                    run_transcript_assembly(
                        contig_acc,
                        contig_strand,
                        contig_seq_str,
                        bam_file_for_sg,
                        bam_file_for_quant,
                        restrict_region_lend,
                        restrict_region_rend,
                        input_transcripts,
                        ofh_gtf,
                        ofh_quant_output,
                        ofh_quant_read_tracker,
                        CPU,
                        run_EM,
                        single_best_only,
                        NO_FILTER_ISOFORMS,
                        min_isoform_fraction,
                        quant_EM_alpha,
                        prereconstruct_info_dir,
                        ME_only,
                        SE_only,
                    )

        # close writers after serial processing
        ofh_gtf.close()
        ofh_quant_output.close()
        ofh_quant_read_tracker.close()
    else:
        # Build contig/strand jobs and run up to CPU in parallel. Each worker writes temp outputs we merge after.
        jobs = []
        for contig_acc in genome_contigs_list:
            for contig_strand in ("+", "-"):
                if (
                    restrict_to_contig_orient is not None
                    and restrict_to_contig_orient != contig_strand
                ):
                    continue
                jobs.append(
                    dict(
                        contig_acc=contig_acc,
                        contig_strand=contig_strand,
                        genome_fasta_filename=genome_fasta_filename,
                        bam_file_for_sg=bam_file_for_sg,
                        bam_file_for_quant=bam_file_for_quant,
                        restrict_region_lend=restrict_region_lend,
                        restrict_region_rend=restrict_region_rend,
                        input_gtf=input_gtf,
                        config_overrides=dict(LRAA_Globals.config),
                        splice_graph_params=splice_graph_params_dict,
                        run_EM=run_EM,
                        single_best_only=single_best_only,
                        NO_FILTER_ISOFORMS=NO_FILTER_ISOFORMS,
                        min_isoform_fraction=min_isoform_fraction,
                        quant_EM_alpha=quant_EM_alpha,
                        prereconstruct_info_dir=prereconstruct_info_dir,
                        ME_only=ME_only,
                        SE_only=SE_only,
                        output_prefix=output_prefix,
                        QUANT_ONLY=QUANT_ONLY,
                        DEBUG=LRAA_Globals.DEBUG,
                        skip=False,
                    )
                )

        # worker function declared at module scope (_contig_job_runner)

        # Run jobs with a Pool of size CPU
        # determine pool size and cores per worker so total cores <= CPU
        pool_size = max(1, min(CPU, len(jobs)))
        CPU_inner = max(1, CPU // pool_size)
        # set per-job CPU
        for j in jobs:
            j["CPU_inner"] = CPU_inner

        logger.info(
            f"Running {len(jobs)} contig/strand jobs with up to {pool_size} concurrent processes; CPU_inner={CPU_inner} (cap={CPU})"
        )

        # Launch in batches of size pool_size to avoid daemonic Pool workers (so inner component multiprocessing can spawn)
        # Before launching, mark jobs already completed (resume) if allowed
        if not NO_RESUME_PARALLEL:
            resumed = 0
            for j in jobs:
                tmp_root = f"{j['output_prefix']}.contigtmp"
                tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
                tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
                ok_path = f"{tmp_prefix}.ok"
                # require all expected outputs to be present
                have_expr = os.path.exists(f"{tmp_prefix}.quant.expr")
                have_track = os.path.exists(f"{tmp_prefix}.quant.tracking")
                have_gtf = True if j["QUANT_ONLY"] else os.path.exists(f"{tmp_prefix}.gtf")
                if os.path.exists(ok_path) and have_expr and have_track and have_gtf:
                    j["skip"] = True
                    resumed += 1
            if resumed:
                logger.info(f"Resuming {resumed} completed contig/strand jobs from previous run")

        procs = []
        job_batches = [jobs[i : i + pool_size] for i in range(0, len(jobs), pool_size)]
        for batch_idx, batch in enumerate(job_batches, start=1):
            logger.info(f"Starting contig/strand batch {batch_idx}/{len(job_batches)} size={len(batch)}")
            procs = []
            for j in batch:
                if j.get("skip"):
                    continue
                p = Process(target=_contig_job_runner, args=(j,))
                p.start()
                procs.append((p, j))
            # join all in batch
            any_fail = False
            for p, j in procs:
                p.join()
                if p.exitcode != 0:
                    logger.error(
                        f"Contig job failed: {j['contig_acc']} {j['contig_strand']} with exit code {p.exitcode}"
                    )
                    any_fail = True
            if any_fail:
                raise RuntimeError("One or more contig/strand jobs failed; see logs above")

        # Merge results
        # Write merged headers
        merged_quant_fh = open(ofh_quant_output_filename, "wt")
        merged_track_fh = open(ofh_quant_read_tracking_filename, "wt")

        # Build and write headers deterministically (DEBUG off for merged header)
        quant_header_final = [
            "gene_id",
            "transcript_id",
            "uniq_reads",
            "all_reads",
            "isoform_fraction",
            "unique_gene_read_fraction",
            "TPM",
            "exons",
            "introns",
            "splice_hash_code",
        ]
        if not QUANT_ONLY:
            quant_header_final += ["splice_compat_contained", "splice_contained_by"]
        print("\t".join(quant_header_final), file=merged_quant_fh)

        tracking_header_final = [
            "gene_id",
            "transcript_id",
            "transcript_splice_hash_code",
            "mp_id",
            "read_name",
            "frac_assigned",
        ]
        if LRAA_Globals.DEBUG:
            tracking_header_final.append("read_weight")
        print("\t".join(tracking_header_final), file=merged_track_fh)

        # Append worker contents (skip first header line)
        def _append_without_first_line(src_path, dst_fh):
            if src_path is None or (not os.path.exists(src_path)):
                return
            with open(src_path, "rt") as fh:
                first = True
                for line in fh:
                    if first:
                        first = False
                        continue
                    dst_fh.write(line)

        for j in jobs:
            tmp_root = f"{j['output_prefix']}.contigtmp"
            tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
            tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
            _append_without_first_line(f"{tmp_prefix}.quant.expr", merged_quant_fh)
            _append_without_first_line(f"{tmp_prefix}.quant.tracking", merged_track_fh)

        merged_quant_fh.close()
        merged_track_fh.close()

        if not QUANT_ONLY:
            # Merge GTFs (no header lines in GTF)
            with open(gtf_output_filename, "wt") as gtf_out:
                for j in jobs:
                    tmp_root = f"{j['output_prefix']}.contigtmp"
                    tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
                    tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
                    gtf_tmp = f"{tmp_prefix}.gtf"
                    if os.path.exists(gtf_tmp):
                        with open(gtf_tmp, "rt") as fh:
                            shutil.copyfileobj(fh, gtf_out)

        # cleanup temp files
        if CLEAN_PARALLEL_TMP:
            for j in jobs:
                tmp_root = f"{j['output_prefix']}.contigtmp"
                tmp_dir = os.path.join(tmp_root, f"{j['contig_acc']}", f"{j['contig_strand']}")
                tmp_prefix = os.path.join(tmp_dir, f"{j['contig_acc']}.{j['contig_strand']}")
                for pth in (
                    f"{tmp_prefix}.quant.expr",
                    f"{tmp_prefix}.quant.tracking",
                    f"{tmp_prefix}.gtf",
                    f"{tmp_prefix}.ok",
                ):
                    if os.path.exists(pth):
                        try:
                            os.remove(pth)
                        except Exception:
                            pass

    # make bed file for convenience
    if not QUANT_ONLY:
        bed_output_file = f"{output_prefix}.bed"
        cmd = " ".join(
            [
                os.path.join(
                    UTILDIR,
                    "gtf_to_bed_format.pl",
                ),
                gtf_output_filename,
                ">",
                bed_output_file,
            ]
        )
        logger.info("-making bed output file: {}".format(bed_output_file))
        subprocess.check_call(cmd, shell=True)

        if INCLUDE_PRELIM_TRANSCRIPTS or LRAA_Globals.DEBUG:
            # write bed for the prelim isoforms too.
            prelim_gtf = "__PRE-Tx-Filtering.transcripts.gtf"
            bed_output_file = "__PRE-Tx-Filtering.transcripts.bed"
            cmd = " ".join(
                [
                    os.path.join(
                        UTILDIR,
                        "gtf_to_bed_format.pl",
                    ),
                    prelim_gtf,
                    ">",
                    bed_output_file,
                ]
            )
            logger.info("-making bed output file: {}".format(bed_output_file))
            subprocess.check_call(cmd, shell=True)

    ##############################################################################
    ## Incorporate read-to-isoform assignment in the aligned bam file (optionally)

    if TAG_BAM:
        cmd = " ".join(
            [
                os.path.join(UTILDIR, "annotate_bam_with_read_tracking_info.py"),
                "--bam",
                bam_file_for_quant,
                "--tracking",
                ofh_quant_read_tracking_filename,
            ]
        )
        logger.info("tagging bam file with transcripts assigned to each read")
        subprocess.check_call(cmd, shell=True)

    return


def run_quant_only(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    bam_file_for_quant,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
    ofh_quant_output,
    ofh_quant_read_tracker,
    CPU,
    run_EM,
    prereconstruct_info_dir,
    report_quants=True,
):

    # get path assignments for the input transcripts
    # get the path assignments for the reads.
    # compare read mappings, assign categories

    # if no transcripts for this contig/strand, nothing to quant on
    if input_transcripts is None:
        logger.info(f"-no isoforms to quant on {contig_acc} [{contig_strand}]")
        # continue
        return

    sg = Splice_graph()

    # for quant only, build sg only based on the input gtf and not the alignments in the bam
    sg.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        input_transcripts,
        quant_mode=True,
        restrict_splice_type=None,
    )

    if sg.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    lraa_obj = LRAA(sg, CPU)
    logger.info("\n//SECTION QUANT: Assigning input transcript paths in graph")
    lraa_obj.assign_transcripts_paths_in_graph(input_transcripts)

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            input_transcripts, "__input_transcripts_path_in_graph"
        )

    logger.info("\n//SECTION QUANT: Assigning reads to paths in graph.")
    mp_counter = lraa_obj._populate_read_multi_paths(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_quant,
        allow_spacers,
        restrict_splice_type=None,
    )

    q = Quantify(
        run_EM, LRAA_Globals.config["max_EM_iterations_quant_only"], quant_mode="final"
    )
    logger.info("\n//SECTION QUANT: Quantifying transcripts according to read support.")
    read_name_to_fractional_transcript_assignment = q.quantify(
        sg, input_transcripts, mp_counter
    )

    if LRAA_Globals.DEBUG:
        q.dump_mp_to_transcripts_to_file(
            "__QUANTIFY_mp_to_transcripts_and_reads.tsv",
            contig_acc,
            contig_strand,
        )

    if report_quants:
        q.report_quant_results(
            input_transcripts,
            read_name_to_fractional_transcript_assignment,
            ofh_quant_output,
            ofh_quant_read_tracker,
        )

    return q


def run_transcript_assembly(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    bam_file_for_quant,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
    ofh_gtf,
    ofh_quant_output,
    ofh_quant_read_tracker,
    CPU,
    run_EM,
    single_best_only,
    NO_FILTER_ISOFORMS,
    min_isoform_fraction,
    quant_EM_alpha,
    prereconstruct_info_dir,
    ME_only,
    SE_only,
):

    ##---------------------
    ## Assemble transcripts

    LRAA_Globals.config["EM_alpha"] = 0.0  # disable during initial asm step.

    LRAA_Globals.LRAA_MODE = "ID-init_norm_reads"

    ##
    ## Multi-exon read processing section
    ##

    ME_input_transcripts = None
    if input_transcripts is not None:
        # extract the ME ones.
        ME_input_transcripts = [x for x in input_transcripts if x.has_introns()]
        if len(ME_input_transcripts) == 0:
            ME_input_transcripts = None

    ME_transcripts = None
    if not SE_only:  # build ME unless explicitly restricted to SE
        ME_transcripts = build_ME_transcripts(
            contig_acc,
            contig_strand,
            contig_seq_str,
            bam_file_for_sg,
            restrict_region_lend,
            restrict_region_rend,
            ME_input_transcripts,
        )

    ##
    ## Single-exon read processing section
    ##

    SE_input_transcripts = None
    if input_transcripts is not None:
        # extract the ME ones.
        SE_input_transcripts = [x for x in input_transcripts if not x.has_introns()]
        if len(SE_input_transcripts) == 0:
            SE_input_transcripts = None

    SE_transcripts = None
    if not ME_only:  # build SE unless explicitly restricted to ME
        SE_transcripts = build_SE_transcripts(
            contig_acc,
            contig_strand,
            contig_seq_str,
            bam_file_for_sg,
            restrict_region_lend,
            restrict_region_rend,
            SE_input_transcripts,
            SE_read_encapsulation_mask=ME_transcripts,
        )

    ##############################
    # Integrate ME and SE isoforms
    all_transcripts = list()
    if ME_transcripts is not None:
        all_transcripts += ME_transcripts
    if SE_transcripts is not None:
        all_transcripts += SE_transcripts

    if len(all_transcripts) == 0:
        return

    ## Build final Splice Graph
    sg_all = Splice_graph()
    logger.info(f"\n// -building splice graph for {contig_acc}")
    transcripts_incl_in_graph = list(all_transcripts)
    if input_transcripts is not None:
        # include only those input transcripts consistent with ME/SE restrictions
        if ME_only:
            transcripts_incl_in_graph += [
                t for t in input_transcripts if t.has_introns()
            ]
        elif SE_only:
            transcripts_incl_in_graph += [
                t for t in input_transcripts if not t.has_introns()
            ]
        else:
            transcripts_incl_in_graph += input_transcripts

    sg_all.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        transcripts_incl_in_graph,
        quant_mode=False,
        restrict_splice_type=None,
    )

    lraa_all_obj = LRAA(sg_all, CPU)

    lraa_all_obj.assign_transcripts_paths_in_graph(
        transcripts_incl_in_graph
    )  # limit to all input transcripts

    # reassign gene and transcript component ids based on shared nodes.
    logger.info("reclustering combined ME and SE transcripts to genes")
    all_transcripts = Transcript.recluster_transcripts_to_genes(
        all_transcripts, contig_acc, contig_strand
    )

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            all_transcripts, "__ME_and_SE_combined_transcripts_path_in_graph"
        )

    ###########################################################
    # Do an initial quant
    # Now use the original bam for quant (not depth normalized)
    LRAA_Globals.LRAA_MODE = "ID-full_reads"

    logger.info(
        "\n//SECTION QUANT using all reads for assembled isoforms: Assigning reads to paths in graph."
    )

    mp_all_counter = lraa_all_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_quant,
        allow_spacers,
        transcripts_incl_in_graph,
        restrict_splice_type=None,
    )

    # Ensure subsequent filtering/quant steps use ONLY real read evidence (not injected reference transcript pseudo-reads)
    # Rationale: earlier build_multipath_graph() calls (with input_transcripts) inject a synthetic "reftranscript:<id>" read per
    # reference transcript to guarantee a path in the graph. These pseudo-reads artificially inflate initial read support counts
    # and can cause inconsistencies vs. the final quant (which rebuilds without reinjecting them). By pruning them here we enforce
    # consistent real-read-based support across all downstream filtering and quantification passes.
    try:
        if mp_all_counter is not None:
            before_prune = (
                sum(
                    [
                        m.get_multipath_and_count()[1]
                        for m in mp_all_counter.get_all_MultiPathCountPairs()
                    ]
                )
                if hasattr(mp_all_counter, "get_all_MultiPathCountPairs")
                else None
            )
            mp_all_counter.prune_ref_transcripts_as_evidence()
            after_prune = (
                sum(
                    [
                        m.get_multipath_and_count()[1]
                        for m in mp_all_counter.get_all_MultiPathCountPairs()
                    ]
                )
                if hasattr(mp_all_counter, "get_all_MultiPathCountPairs")
                else None
            )
            if before_prune is not None and after_prune is not None:
                logger.info(
                    f"Pruned reference transcript pseudo-read evidence: total mp read counts {before_prune} -> {after_prune}"
                )
            else:
                logger.info(
                    "Pruned reference transcript pseudo-read evidence from multipath counter"
                )
    except Exception as e:
        logger.warning(f"Failed pruning reference transcript pseudo-read evidence: {e}")

    q = Quantify(
        run_EM,
        LRAA_Globals.config["max_EM_iterations_during_asm"],
        quant_mode="draft",
    )
    frac_read_assignments = q.quantify(sg_all, all_transcripts, mp_all_counter)

    if INCLUDE_PRELIM_TRANSCRIPTS or LRAA_Globals.DEBUG:

        with open("__PRE-Tx-Filtering.transcripts.gtf", "at") as ofh:
            for transcript in all_transcripts:
                ofh.write(transcript.to_GTF_format(include_TPM=False) + "\n")

        with open("__PRE-Tx-Filtering.quant.expr", "at") as ofh_prefilter_quant_expr:
            with open(
                "__PRE-Tx-Filtering.quant.tracking", "at"
            ) as ofh_prefilter_quant_tracking:
                q.report_quant_results(
                    all_transcripts,
                    frac_read_assignments,
                    ofh_prefilter_quant_expr,
                    ofh_prefilter_quant_tracking,
                )

    ##########################################
    ## Filtering of isoforms to remove 'noise'
    #########################################

    transcripts = all_transcripts  # legacy varname

    if input_transcripts is not None:
        # build splice pattern hash set for spliced reference transcripts only
        ref_splice_hashes = set(
            [
                Util_funcs.get_hash_code(t.get_introns_string())
                for t in input_transcripts
                if t.has_introns()
            ]
        )
        LRAA.differentiate_known_vs_novel_isoforms(
            transcripts, reference_splice_hashes=ref_splice_hashes
        )

    if NO_FILTER_ISOFORMS:

        logger.info("NOT FILTERING ISOFORMS.")

    else:

        ##################################################################
        ## initial filter of novel isoforms with insufficient read support

        logger.info("\n// Filtering novel isoforms by min read support")
        logger.info("Before filtering, have {} transcripts".format(len(transcripts)))
        min_read_support_novel_isoforms = LRAA_Globals.config["min_reads_novel_isoform"]
        transcripts = TranscriptFiltering.filter_novel_isoforms_by_min_read_support(
            transcripts, min_read_support_novel_isoforms
        )
        logger.info("After filtering, have {} transcripts".format(len(transcripts)))

        if len(transcripts) < 1:
            logger.info(
                "-no transcripts on contig {} {} survived min novel isoform read threshold requirement".format(
                    contig_acc, contig_strand
                )
            )
            # continue
            return

        # rerun quant for post-filtering
        logger.info(
            "\n// -rerunning quant post filtering novel isoforms via read thresholds"
        )
        frac_read_assignments = q.quantify(sg_all, transcripts, mp_all_counter)

        #####################################
        # pruning likely degradation products

        logger.info("\n// -pruning likely degradation products")
        logger.info("Before pruning, have {} transcripts".format(len(transcripts)))

        transcripts = TranscriptFiltering.prune_likely_degradation_products(
            transcripts, sg_all, frac_read_assignments
        )
        logger.info("After pruning, have {} transcripts".format(len(transcripts)))

        if len(transcripts) < 1:
            logger.info(
                "-no transcripts on contig {} {} survived pruning degradation products".format(
                    contig_acc, contig_strand
                )
            )
            # continue
            return

        # rerun quant for post-filtering
        logger.info("\n// -rerunning quant post-filtering of degradation products")
        frac_read_assignments = q.quantify(sg_all, transcripts, mp_all_counter)

        #######################################################
        # Filter isoforms according to minimum isoform fraction

        if min_isoform_fraction > 0:
            logger.info(
                "\n// -filtering isoforms according to minimum isoform fraction"
            )
            logger.info(
                "Before filtering, have {} transcripts".format(len(transcripts))
            )
            transcripts = TranscriptFiltering.filter_isoforms_by_min_isoform_fraction(
                transcripts,
                min_isoform_fraction,
                run_EM,
                LRAA_Globals.config["max_EM_iterations_during_asm"],
            )
            logger.info("After filtering, have {} transcripts".format(len(transcripts)))

            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived isoform filtering".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

        ##################################################################
        # Filter monoexonic transcripts according to expression thresholds
        if LRAA_Globals.config["min_monoexonic_TPM"] > 0:
            logger.info("\n// -filtering monoexonic transcripts based on min TPM")
            logger.info(
                "Before filtering, have {} transcripts".format(len(transcripts))
            )
            transcripts = (
                TranscriptFiltering.filter_monoexonic_isoforms_by_TPM_threshold(
                    transcripts, LRAA_Globals.config["min_monoexonic_TPM"]
                )
            )
            logger.info("After filtering, have {} transcripts".format(len(transcripts)))
            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived monoexonic filtering strategy".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

        ####################################################################
        # Filter isoforms apparently derived from internal priming artifacts
        if LRAA_Globals.config["filter_internal_priming"]:
            logger.info("\n// -filtering out internal priming")
            logger.info(
                "Before filtering, have {} transcripts".format(len(transcripts))
            )
            transcripts = TranscriptFiltering.filter_internally_primed_transcripts(
                transcripts,
                contig_seq_str,
                contig_strand,
                input_transcripts,
                restrict_filter_to_monoexonic=LRAA_Globals.config[
                    "restrict_internal_priming_filter_to_monoexonic"
                ],
            )
            logger.info("After filtering, have {} transcripts".format(len(transcripts)))
            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived internal priming filtering".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            transcripts, "__output_transcripts_path_in_graph"
        )

    ###################
    ## Final quant step
    ###################

    debug_mode_setting = LRAA_Globals.DEBUG
    if debug_mode_setting is True:
        LRAA_Globals.DEBUG = False

    LRAA_Globals.config["EM_alpha"] = quant_EM_alpha  # restore for final quant step.

    logger.info("Running final quant for {} transcripts".format(len(transcripts)))

    q = run_quant_only(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        bam_file_for_quant,
        restrict_region_lend,
        restrict_region_rend,
        transcripts,
        ofh_quant_output,
        ofh_quant_read_tracker,
        CPU,
        run_EM,
        prereconstruct_info_dir,
        report_quants=False,
    )

    # reset
    LRAA_Globals.DEBUG = debug_mode_setting

    ## one last filtering based on min isoform fraction

    transcripts_kept, transcripts_removed = (
        remove_low_isoform_fraction_transcripts_final_attempt(
            transcripts, min_isoform_fraction
        )
    )

    if len(transcripts_removed) > 0:
        transcripts = transcripts_kept

    frac_read_assignments = q._estimate_isoform_read_support(
        transcripts
    )  ##//TODO: make frac read assignments a member of the quantifier obj. Shouldn't need to run it again here unless transcripts got filtered above.

    ###############################################
    # examine splice compatible isoform differences (note, not currently filtering here, just annotating feature)
    (
        transcript_splice_compatible_containments,
        transcript_splice_compatible_contained_by,
    ) = TranscriptFiltering.evaluate_splice_compatible_alt_isoforms(transcripts)

    q.report_quant_results(
        transcripts,
        frac_read_assignments,
        ofh_quant_output,
        ofh_quant_read_tracker,
        splice_compatible_containments=transcript_splice_compatible_containments,
        splice_compatible_contained_by=transcript_splice_compatible_contained_by,
    )

    #######################################
    ## Done filtering, report final results

    ## report transcripts in GTF format
    logger.info(
        "writing gtf output for {} [{}] containing {} transcripts".format(
            contig_acc, contig_strand, len(transcripts)
        )
    )

    for transcript in transcripts:
        ofh_gtf.write(transcript.to_GTF_format() + "\n")

    return


def remove_low_isoform_fraction_transcripts_final_attempt(
    transcripts, min_isoform_fraction
):

    transcripts_kept = list()
    transcripts_removed = list()

    for transcript in transcripts:
        if transcript.get_isoform_fraction() < min_isoform_fraction:
            transcripts_removed.append(transcript)
        else:
            transcripts_kept.append(transcript)

    return transcripts_kept, transcripts_removed


def get_genome_contigs_listing(genome_fasta_filename):

    fai_file = "{}.fai".format(genome_fasta_filename)
    if not os.path.exists(fai_file):
        subprocess.check_call(
            "samtools faidx {}".format(genome_fasta_filename), shell=True
        )

    contigs_list = list()

    with open(fai_file) as fh:
        for line in fh:
            vals = line.split("\t")
            contig_acc = vals[0]
            contigs_list.append(contig_acc)

    return contigs_list


def report_transcript_paths_in_graph(transcripts, filename):

    with open(filename, "at") as ofh:
        for transcript in transcripts:
            sp = transcript.get_simple_path()
            exon_segments = transcript.get_exon_segments()
            transcript_id = transcript.get_transcript_id()
            print("\t".join([transcript_id, str(exon_segments), str(sp)]), file=ofh)

    return


def count_reads_from_bam(bam_filename):

    read_count_file = os.path.basename(bam_filename) + ".count"
    if not os.path.exists(read_count_file):
        subprocess.check_call(
            f"samtools view -c {bam_filename} > {read_count_file}.tmp", shell=True
        )
        os.rename(f"{read_count_file}.tmp", read_count_file)

    with open(read_count_file, "rt") as fh:
        count = next(fh)
        count = count.rstrip()
        count = int(count)
        assert count > 0, "Error, no reads counted from bam file..."
        return count


def build_ME_transcripts(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
):

    sg_ME = Splice_graph()

    ## Build ME Splice Graph
    logger.info(f"\n// -building splice graph for {contig_acc}")
    sg_ME.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        input_transcripts,
        quant_mode=False,
        restrict_splice_type="ME",
    )

    if sg_ME.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg_ME.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    # Incorporate reference transcripts if provided
    lraa_ME_obj = LRAA(sg_ME, CPU)

    if input_transcripts:
        lraa_ME_obj.assign_transcripts_paths_in_graph(
            input_transcripts
        )  # limit to ME input transcripts
        if LRAA_Globals.DEBUG:
            report_transcript_paths_in_graph(
                input_transcripts, "__input_transcripts_path_in_graph"
            )

    # build graph path nodes based on reads (and input transcripts) paths through graph.
    logger.info(f"\n// -building multpath graph for {contig_acc} {contig_strand}")
    mp_ME_counter = lraa_ME_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        allow_spacers,
        input_transcripts,
        restrict_splice_type="ME",
    )

    # Define ME isoforms
    logger.info(f"\n// -begin reconstructing isoforms for {contig_acc}")
    ME_transcripts = lraa_ME_obj.reconstruct_isoforms(single_best_only)

    if len(ME_transcripts) == 0:
        logger.info(
            "no ME transcripts constructed on {} {}".format(contig_acc, contig_strand)
        )

        return None

    else:
        with open("__ME_isoforms.gtf", "at") as tmp_ofh:
            for transcript in ME_transcripts:
                tmp_ofh.write(transcript.to_GTF_format() + "\n")

        return ME_transcripts


def build_SE_transcripts(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    restrict_region_lend,
    restrict_region_rend,
    transcripts_incl_in_graph,
    SE_read_encapsulation_mask,
):

    sg_SE = Splice_graph()

    ## Build SE Splice Graph

    logger.info(f"\n// -building splice graph for {contig_acc}")
    sg_SE.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        transcripts_incl_in_graph,
        quant_mode=False,
        restrict_splice_type="SE",
        SE_read_encapsulation_mask=SE_read_encapsulation_mask,
    )

    if sg_SE.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg_SE.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    # Incorporate reference transcripts if provided
    lraa_SE_obj = LRAA(sg_SE, CPU)

    if transcripts_incl_in_graph:
        lraa_SE_obj.assign_transcripts_paths_in_graph(transcripts_incl_in_graph)
        if LRAA_Globals.DEBUG:
            report_transcript_paths_in_graph(
                transcripts_incl_in_graph, "__SE_incl_transcripts_path_in_graph"
            )

    # build graph path nodes based on reads (and input transcripts) paths through graph.
    logger.info(f"\n// -building multpath graph for {contig_acc} {contig_strand}")
    mp_SE_counter = lraa_SE_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        allow_spacers,
        transcripts_incl_in_graph,
        restrict_splice_type="SE",
        SE_read_encapsulation_mask=SE_read_encapsulation_mask,
    )

    #####################
    # Define SE isoforms

    logger.info(f"\n// -begin reconstructing isoforms for {contig_acc}")
    SE_transcripts = lraa_SE_obj.reconstruct_isoforms(single_best_only)

    if len(SE_transcripts) == 0:
        logger.info(
            "no SE transcripts constructed on {} {}".format(contig_acc, contig_strand)
        )
        # continue
        return None

    else:
        with open("__SE_isoforms.gtf", "at") as tmp_ofh:
            for transcript in SE_transcripts:
                tmp_ofh.write(transcript.to_GTF_format() + "\n")

        return SE_transcripts


if __name__ == "__main__":
    main()
