#!/usr/bin/env python3
# encoding: utf-8

import sys, os, re
import pysam
import argparse
import subprocess
from collections import defaultdict

sys.path.insert(
    0, os.path.sep.join([os.path.dirname(os.path.realpath(__file__)), "pylib"])
)
from Splice_graph import Splice_graph
from LRAA import LRAA
from Transcript import Transcript, GTF_contig_to_transcripts
from Quantify import Quantify
import TranscriptFiltering
import Util_funcs
import LRAA_Globals
import MultiProcessManager as mpm
import logging
import lmdb
import shutil


####################################
# VERSION=""
VERSION = "BLEEDING_EDGE-v0.2.30"
####################################


# FORMAT = "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s at %(lineno)d :\n\t%(message)s\n"
FORMAT = (
    "%(asctime)-15s %(levelname)s %(module)s.%(name)s.%(funcName)s:\n\t%(message)s\n"
)

logger = logging.getLogger()
logging.basicConfig(format=FORMAT, level=logging.INFO)


num_threads = 4
allow_spacers = False


def main():

    parser = argparse.ArgumentParser(
        description="LRAA: Long Read Alignment Assembler",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument("--bam", type=str, required=True, help="target bam file")
    parser.add_argument("--genome", type=str, required=True, help="target genome file")

    parser.add_argument(
        "--gtf",
        type=str,
        required=False,
        help="GTF to incorporate as guide during reconstruction or for targeting quant-only",
    )

    parser.add_argument(
        "--output_prefix", type=str, default="LRAA", help="prefix for output filenames"
    )

    parser.add_argument(
        "--single_best_only",
        action="store_true",
        default=False,
        help="only report the single highest scoring isoform per component",
    )

    parser.add_argument(
        "--CPU",
        type=int,
        default=num_threads,
        help="number of cores for multithreading (default: {})".format(num_threads),
    )

    parser.add_argument(
        "--no_norm",
        action="store_true",
        default=False,
        help="do not run read coverage normalization before isoform reconstruction (equivalent to --normalize_max_cov_level 0)",
    )

    parser.add_argument(
        "--normalize_max_cov_level",
        type=int,
        default=LRAA_Globals.config["normalize_max_cov_level"],
        help="normalize to max read coverage level before assembly (default: {})".format(
            LRAA_Globals.config["normalize_max_cov_level"]
        ),
    )

    parser.add_argument(
        "--quant_only",
        action="store_true",
        default=False,
        help="perform quantification only (must specify --gtf with targets for quant)",
    )

    parser.add_argument(
        "--tag_bam",
        action="store_true",
        default=False,
        help="if performing quant, also output a copy of the BAM with additional tags containing classification and quant",
    )

    parser.add_argument(
        "--no_EM",
        action="store_true",
        default=False,
        help="do not run EM alg instead simply divide uncertain mappings equally among assigned target isoforms",
    )

    parser.add_argument(
        "--LowFi",
        action="store_true",
        default=False,
        help="low fidelity mode: min_per_id: 80, no_infer_TSS, no_infer_PolyA, max_exon_spur_length: 20",
    )

    parser.add_argument(
        "--version",
        action="store_true",
        default=False,
        help="display version: {}".format(VERSION),
    )

    ## debug params

    debug_group = parser.add_argument_group("debug settings")

    debug_group.add_argument(
        "--debug",
        "-d",
        action="store_true",
        default=False,
        help="debug mode, more verbose",
    )

    debug_group.add_argument(
        "--mpm_monitor", action="store_true", default=False
    )  # multiprocessing monitor

    debug_group.add_argument(
        "--no_filter_isoforms",
        action="store_true",
        default=False,
        help="do not filter any initially resolved isoforms",
    )

    ## config settings

    config_group = parser.add_argument_group("config settings")

    # disabling spacers for now - important for illumina or dirty long reads
    # config_group.add_argument("--allow_spacers", action='store_true', default=False)

    config_group.add_argument(
        "--num_total_reads",
        "-N",
        type=int,
        default=None,
        help="total number of reads for use in TPM calculations",
    )

    config_group.add_argument(
        "--min_path_score",
        type=float,
        default=LRAA_Globals.config["min_path_score"],
        help="minimum score for an isoform to be reported. default({})".format(
            LRAA_Globals.config["min_path_score"]
        ),
    )

    config_group.add_argument(
        "--min_per_id",
        type=float,
        default=LRAA_Globals.config["min_per_id"],
        help="min per_id for pacbio read alignments. default: ({})".format(
            LRAA_Globals.config["min_per_id"]
        ),
    )

    config_group.add_argument(
        "--max_intron_length",
        type=int,
        default=LRAA_Globals.config["max_intron_length"],
        help="maximum allowable intron length. default: ({})".format(
            LRAA_Globals.config["max_intron_length"]
        ),
    )

    config_group.add_argument(
        "--min_mapping_quality",
        type=int,
        default=LRAA_Globals.config["min_mapping_quality"],
        help="minimum read alignment mapping quality (default: {})".format(
            LRAA_Globals.config["min_mapping_quality"]
        ),
    )

    config_group.add_argument(
        "--min_isoform_fraction",
        type=float,
        default=LRAA_Globals.config["min_isoform_fraction"],
        help="exclude reconstructed isoforms that have read support less than this fraction of all isoforms at each gene (no impact on --quant_only). default: ({})".format(
            LRAA_Globals.config["min_isoform_fraction"]
        ),
    )

    config_group.add_argument(
        "--no_infer_TSS", action="store_true", default=False, help="do not infer TSS"
    )
    config_group.add_argument(
        "--no_infer_PolyA",
        action="store_true",
        default=False,
        help="do not infer_PolyA",
    )

    config_group.add_argument(
        "--min_monoexonic_TPM",
        type=float,
        required=False,
        default=LRAA_Globals.config["min_monoexonic_TPM"],
        help="minimum TPM for mono-exonic isoforms (default: {})".format(
            LRAA_Globals.config["min_monoexonic_TPM"]
        ),
    )

    config_group.add_argument(
        "--no_filter_internal_priming",
        action="store_true",
        default=False,
        help="do not filter isoforms that appear to derive from internal priming events",
    )

    config_group.add_argument(
        "--no_use_weighted_read_assignments",
        action="store_true",
        default=False,
        help="do not weight reads according to agreement with start/end of reads",
    )

    config_group.add_argument(
        "--ref_trans_filter_mode",
        default="retain_expressed",
        choices=["retain_expressed", "retain_filtered"],
        help="logic around retaining input reference transcripts in guided ID mode",
    )

    ## EM settings
    config_group_EM = parser.add_argument_group("EM settings")

    # config_group_EM.add_argument(
    #    "--EM_implementation",
    #    default="CGPT",
    #    choices=["CGPT", "BJH"],
    #    help="implementation of the EM alg to use.",
    # )

    config_group_EM.add_argument(
        "--EM_alpha",
        type=float,
        default=0.01,
        required=False,
        help="regularization factor EM_alpha * num_ambigous_reads for each transcript",
    )

    ## alt splice settings

    config_group_altsplice = parser.add_argument_group("alt splice settings")

    # TODO:// mv splice defaults to globals.config
    config_group_altsplice.add_argument(
        "--min_alt_splice_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_splice_freq"],
        help="min fraction required for alt splicing at an exon boundary (default: {})".format(
            LRAA_Globals.config["min_alt_splice_freq"]
        ),
    )

    config_group_altsplice.add_argument(
        "--min_alt_unspliced_freq",
        type=float,
        default=LRAA_Globals.config["min_alt_unspliced_freq"],
        help="min fraction required for retained intron at splice boundary (default: {})".format(
            LRAA_Globals.config["min_alt_unspliced_freq"]
        ),
    )

    ## restrict to contig and optionally region of contig

    contig_group_setting = parser.add_argument_group(
        "target specific contig (or region of contig)"
    )

    contig_group_setting.add_argument(
        "--contig", type=str, default=None, help="restrict run to single contig"
    )
    contig_group_setting.add_argument(
        "--region",
        type=str,
        default=None,
        help="restrict to region on contig chr\\d+[+-]?:\\d+-\\d+  ex. chr2:12345-56789 or chr2+:12345-56789 to restrict to the top strand",
    )

    if "--version" in sys.argv:
        print("LRAA VERSION: {}".format(VERSION))
        if len(sys.argv) == 2:
            sys.exit(0)

    args = parser.parse_args()

    genome_fasta_filename = args.genome
    bam_filename = args.bam
    output_prefix = args.output_prefix
    single_best_only = args.single_best_only
    CPU = args.CPU
    QUANT_ONLY = args.quant_only
    TAG_BAM = args.tag_bam
    input_gtf = args.gtf
    NO_NORM = args.no_norm
    min_isoform_fraction = args.min_isoform_fraction
    run_EM = not args.no_EM

    NO_FILTER_ISOFORMS = args.no_filter_isoforms

    # if args.collapse:
    #    LRAA_Globals.config['collapse_alt_TSS_and_PolyA'] = True

    if args.quant_only and input_gtf is None:
        sys.exit(
            "If running --quant_only, must specify --gtf corresponding to targets of quantification"
        )

    ###############
    # update config
    LRAA_Globals.config["min_path_score"] = args.min_path_score
    LRAA_Globals.config["min_per_id"] = args.min_per_id
    LRAA_Globals.config["max_intron_length"] = args.max_intron_length
    LRAA_Globals.config["min_mapping_quality"] = args.min_mapping_quality
    LRAA_Globals.config["min_isoform_fraction"] = args.min_isoform_fraction
    LRAA_Globals.config["infer_TSS"] = not args.no_infer_TSS
    LRAA_Globals.config["infer_PolyA"] = not args.no_infer_PolyA
    LRAA_Globals.config["min_monoexonic_TPM"] = args.min_monoexonic_TPM
    LRAA_Globals.config["run_EM"] = run_EM
    LRAA_Globals.config["filter_internal_priming"] = not args.no_filter_internal_priming
    LRAA_Globals.config["use_weighted_read_assignments"] = (
        not args.no_use_weighted_read_assignments
    )
    LRAA_Globals.config["EM_implementation_use"] = "CGPT"  # args.EM_implementation
    LRAA_Globals.config["ref_trans_filter_mode"] = args.ref_trans_filter_mode

    quant_EM_alpha = args.EM_alpha
    LRAA_Globals.config["EM_alpha"] = quant_EM_alpha

    if args.LowFi:
        LRAA_Globals.config["infer_TSS"] = False
        LRAA_Globals.config["infer_PolyA"] = False
        LRAA_Globals.config["min_per_id"] = 80.0
        LRAA_Globals.config["max_exon_spur_length"] = 20
        LRAA_Globals.config["fracture_splice_graph_at_input_transcript_bounds"] = False
        LRAA_Globals.config["aggregate_adjacent_splice_boundaries"] = True

    if args.mpm_monitor:
        mpm.set_debug()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        LRAA_Globals.DEBUG = True

    allow_spacers = False
    # allow_spacers = args.allow_spacers

    if not os.path.exists(bam_filename + ".bai"):
        logger.info(
            "-missing index for bam {}, will try to make it...".format(bam_filename)
        )
        subprocess.check_call("samtools index " + bam_filename, shell=True)

    ## perform normalization prior to assembly
    pre_norm_bam_filename = bam_filename

    bam_file_for_quant = pre_norm_bam_filename
    bam_file_for_sg = pre_norm_bam_filename

    if args.num_total_reads is None:
        LRAA_Globals.config["num_total_reads"] = count_reads_from_bam(
            bam_file_for_quant
        )
        logger.info(
            "total number of reads counted from {} as: {}".format(
                bam_file_for_quant, LRAA_Globals.config["num_total_reads"]
            )
        )

    else:
        LRAA_Globals.config["num_total_reads"] = args.num_total_reads
        logger.info("using total number of reads as: {}".format(args.num_total_reads))

    if (not QUANT_ONLY) and (not NO_NORM) and (args.normalize_max_cov_level > 0):

        SS_bamsifter_prog = os.path.sep.join(
            [
                os.path.dirname(os.path.realpath(__file__)),
                "util/normalize_bam_by_strand.py",
            ]
        )

        norm_bam_filename = (
            os.path.basename(bam_filename) + f".norm_{args.normalize_max_cov_level}.bam"
        )

        cmd = " ".join(
            [
                SS_bamsifter_prog,
                f" --input_bam {bam_filename}",
                f" --normalize_max_cov_level {args.normalize_max_cov_level} ",
                f" --output_bam {norm_bam_filename} ",
            ]
        )

        norm_bam_checkpoint = norm_bam_filename + ".ok"
        if not os.path.exists(norm_bam_checkpoint):
            logger.info("generating normalized bam file")
            logger.info(cmd)
            subprocess.check_call(cmd, shell=True)

            subprocess.check_call(f"touch {norm_bam_checkpoint}", shell=True)

        bam_file_for_sg = norm_bam_filename

    ###########################################
    ## on to isoform discovery / reconstruction

    splice_graph_params_dict = {
        "read_aln_gap_merge_int": 10,
        "inter_exon_segment_merge_dist": 50,
        "max_genomic_contig_length": 1e10,
        "min_alt_splice_freq": args.min_alt_splice_freq,
        "min_alt_unspliced_freq": args.min_alt_unspliced_freq,
        "max_intron_length_for_exon_segment_filtering": 10000,
        "min_intron_support": 2,
        "min_terminal_splice_exon_anchor_length": 15,
        "remove_unspliced_introns": False,
    }
    # TODO:// put above settings into the config and add to cmd line interface

    Splice_graph.init_sg_params(splice_graph_params_dict)

    # data structures want:
    # ultimately exons and introns
    # build from reads.
    # define coverage intervals and introns as graph components.

    restricted_contig = None
    restrict_to_contig_orient = None

    if args.contig:
        restricted_contig = args.contig
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-])?$", restricted_contig)
        if m is not None:
            restricted_contig = m.group(1)
            restrict_to_contig_orient = m.group(2)

        genome_contigs_list = [restricted_contig]
    else:
        genome_contigs_list = get_genome_contigs_listing(genome_fasta_filename)

    restrict_region_lend, restrict_region_rend = None, None
    if args.region:
        args.region = args.region.replace(",", "")
        m = re.search("^(\\S*[^\\+\\-])([\\+\\-])?:(\\d+)-(\\d+)$", args.region)
        if m is None:
            raise RuntimeError(
                "Error, could not parse region range: {}".format(args.region)
            )

        restricted_contig = m.group(1)

        genome_contigs_list = [restricted_contig]

        restrict_to_contig_orient = m.group(2)
        if restrict_to_contig_orient not in ("+", "-"):
            restrict_to_contig_orient = None

        restrict_region_lend = m.group(3)
        restrict_region_rend = m.group(4)

        restrict_region_lend = int(restrict_region_lend)
        restrict_region_rend = int(restrict_region_rend)
        assert (
            restrict_region_lend < restrict_region_rend
        ), f"Error, {args.region} invalid range"

        print(
            "{}\t{}\t{}\t{}".format(
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )

    prereconstruct_info_dir = "__prereconstruct"
    if LRAA_Globals.DEBUG:
        if not os.path.exists(prereconstruct_info_dir):
            os.makedirs(prereconstruct_info_dir)

    contig_strand_to_input_transcripts = defaultdict(list)
    if input_gtf:
        logger.info(f"-capturing input transcripts from gtf {input_gtf}")
        contig_to_input_transcripts = (
            GTF_contig_to_transcripts.parse_GTF_to_Transcripts(
                input_gtf,
                restricted_contig,
                restrict_to_contig_orient,
                restrict_region_lend,
                restrict_region_rend,
            )
        )
        for contig, transcript_obj_list in contig_to_input_transcripts.items():
            for transcript in transcript_obj_list:
                transcript_strand = transcript.get_strand()
                contig_strand_token = "{}^{}".format(contig, transcript_strand)
                contig_strand_to_input_transcripts[contig_strand_token].append(
                    transcript
                )

    ##----------------------------------------------
    ## Begin, target each contig separately

    ofh_quant_read_tracker = None
    ofh_quant_read_tracking_lmdb = None

    ofh_quant_output_filename = f"{output_prefix}.quant.expr"
    ofh_quant_output = open(ofh_quant_output_filename, "wt")

    ofh_quant_read_tracking_filename = f"{output_prefix}.quant.tracking"
    ofh_quant_read_tracker = open(ofh_quant_read_tracking_filename, "wt")
    # write header
    print(
        "\t".join(
            [
                "gene_id",
                "transcript_id",
                "read_name",
                "frac_assigned",
                "read_weight",
            ]
        ),
        file=ofh_quant_read_tracker,
    )

    if TAG_BAM:
        ofh_quant_read_tracking_lmdb_filename = f"{output_prefix}.quant.tracking.lmdb"
        # since the LMDB doesn't get reset automatically, make sure there isn't old data leftover
        if os.path.exists(ofh_quant_read_tracking_lmdb_filename):
            shutil.rmtree(ofh_quant_read_tracking_lmdb_filename)
        ofh_quant_read_tracking_lmdb = lmdb.open(
            ofh_quant_read_tracking_lmdb_filename,
            map_size=int(1e11),
            metasync=False,
            sync=False,
        )

    gtf_output_filename = f"{output_prefix}.gtf"

    ofh_gtf = open(gtf_output_filename, "wt")

    # write header
    header = [
        "gene_id",
        "transcript_id",
        "uniq_reads",
        "all_reads",
        "isoform_fraction",
        "unique_gene_read_fraction",
        "TPM",
        "exons",
        "introns",
    ]

    if not QUANT_ONLY:
        header += ["splice_compat_contained", "splice_contained_by"]

    print(
        "\t".join(header),
        file=ofh_quant_output,
    )

    for contig_acc in genome_contigs_list:

        contig_seq_str = Util_funcs.retrieve_contig_seq_from_fasta_file(
            contig_acc, genome_fasta_filename
        )

        for contig_strand in ("+", "-"):

            if (
                restrict_to_contig_orient is not None
                and restrict_to_contig_orient != contig_strand
            ):
                continue

            logger.info(f"-processing contig: {contig_acc} {contig_strand}")

            ##-------------------
            ## build splice graph

            contig_strand_token = "{}^{}".format(contig_acc, contig_strand)
            input_transcripts = contig_strand_to_input_transcripts[contig_strand_token]
            logger.info(
                "Have {} transcripts on {}".format(
                    len(input_transcripts), contig_strand_token
                )
            )

            if input_transcripts is not None and len(input_transcripts) == 0:
                input_transcripts = None

            if QUANT_ONLY:

                run_quant_only(
                    contig_acc,
                    contig_strand,
                    contig_seq_str,
                    bam_file_for_sg,
                    bam_file_for_quant,
                    restrict_region_lend,
                    restrict_region_rend,
                    input_transcripts,
                    ofh_quant_output,
                    ofh_quant_read_tracker,
                    ofh_quant_read_tracking_lmdb,
                    CPU,
                    run_EM,
                    prereconstruct_info_dir,
                    report_quants=True,
                )

            else:
                run_transcript_assembly(
                    contig_acc,
                    contig_strand,
                    contig_seq_str,
                    bam_file_for_sg,
                    bam_file_for_quant,
                    restrict_region_lend,
                    restrict_region_rend,
                    input_transcripts,
                    ofh_gtf,
                    ofh_quant_output,
                    ofh_quant_read_tracker,
                    ofh_quant_read_tracking_lmdb,
                    CPU,
                    run_EM,
                    single_best_only,
                    NO_FILTER_ISOFORMS,
                    min_isoform_fraction,
                    quant_EM_alpha,
                    prereconstruct_info_dir,
                )

    ofh_gtf.close()
    ofh_quant_output.close()
    ofh_quant_read_tracker.close()

    ##############################################################################
    ## Incorporate read-to-isoform assignment in the aligned bam file (optionally)

    if TAG_BAM:
        # explicitly delete the writeable lmdb environment to close it since there is no .close()
        del ofh_quant_read_tracking_lmdb

        # reopen in read-only mode
        ofh_quant_read_tracking_lmdb = lmdb.open(
            ofh_quant_read_tracking_lmdb_filename, readonly=True, lock=False
        )
        output_bam_filename = f"{output_prefix}.tagged.bam"

        with ofh_quant_read_tracking_lmdb.begin(
            write=False
        ) as txn, pysam.AlignmentFile(
            bam_filename, "rb", threads=4
        ) as bam_in, pysam.AlignmentFile(
            output_bam_filename, "wb", template=bam_in, threads=4
        ) as bam_out:
            for read in bam_in.fetch(until_eof=True):
                data = txn.get(read.query_name.encode("utf-8"))
                if data:
                    row = data.decode("utf-8").split(",")
                    if read.has_tag("XG") or read.has_tag("XI"):
                        raise ValueError(
                            "Error, read already has XG or XI tag, so cannot set them without overwriting"
                        )

                    gene_ids = ",".join(row[i] for i in range(0, len(row), 3))
                    transcript_ids = ",".join(row[i + 1] for i in range(0, len(row), 3))
                    frac_assigned = ",".join(row[i + 2] for i in range(0, len(row), 3))

                    read.set_tag("XG", gene_ids, "Z")
                    read.set_tag("XI", transcript_ids, "Z")
                    read.set_tag("XF", frac_assigned, "Z")

                bam_out.write(read)

        ofh_quant_read_tracking_lmdb.close()

    return


def run_quant_only(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    bam_file_for_quant,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
    ofh_quant_output,
    ofh_quant_read_tracker,
    ofh_quant_read_tracking_lmdb,
    CPU,
    run_EM,
    prereconstruct_info_dir,
    report_quants=True,
):

    # get path assignments for the input transcripts
    # get the path assignments for the reads.
    # compare read mappings, assign categories

    # if no transcripts for this contig/strand, nothing to quant on
    if input_transcripts is None:
        logger.info(f"-no isoforms to quant on {contig_acc} [{contig_strand}]")
        # continue
        return

    sg = Splice_graph()

    # for quant only, build sg only based on the input gtf and not the alignments in the bam
    sg.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        input_transcripts,
        quant_mode=True,
    )

    if sg.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    lraa_obj = LRAA(sg, CPU)
    logger.info("\n//SECTION QUANT: Assigning input transcript paths in graph")
    lraa_obj.assign_transcripts_paths_in_graph(input_transcripts)

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            input_transcripts, "__input_transcripts_path_in_graph"
        )

    logger.info("\n//SECTION QUANT: Assigning reads to paths in graph.")
    mp_counter = lraa_obj._populate_read_multi_paths(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_quant,
        allow_spacers,
    )

    q = Quantify(
        run_EM, LRAA_Globals.config["max_EM_iterations_quant_only"], quant_mode="final"
    )
    logger.info("\n//SECTION QUANT: Quantifying transcripts according to read support.")
    read_name_to_fractional_transcript_assignment = q.quantify(
        sg, input_transcripts, mp_counter
    )

    if LRAA_Globals.DEBUG:
        q.dump_mp_to_transcripts_to_file(
            "__QUANTIFY_mp_to_transcripts_and_reads.tsv",
            contig_acc,
            contig_strand,
        )

    if report_quants:
        q.report_quant_results(
            input_transcripts,
            read_name_to_fractional_transcript_assignment,
            ofh_quant_output,
            ofh_quant_read_tracker,
            ofh_quant_read_tracking_lmdb,
        )

    return q


def run_transcript_assembly(
    contig_acc,
    contig_strand,
    contig_seq_str,
    bam_file_for_sg,
    bam_file_for_quant,
    restrict_region_lend,
    restrict_region_rend,
    input_transcripts,
    ofh_gtf,
    ofh_quant_output,
    ofh_quant_read_tracker,
    ofh_quant_read_tracking_lmdb,
    CPU,
    run_EM,
    single_best_only,
    NO_FILTER_ISOFORMS,
    min_isoform_fraction,
    quant_EM_alpha,
    prereconstruct_info_dir,
):

    ##---------------------
    ## Assemble transcripts

    LRAA_Globals.config["EM_alpha"] = 0.0  # disable during initial asm step.

    sg = Splice_graph()

    ## Build Splice Graph
    logger.info(f"\n// -building splice graph for {contig_acc}")
    sg.build_splice_graph_for_contig(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        restrict_region_lend,
        restrict_region_rend,
        input_transcripts,
        quant_mode=False,
    )

    if sg.is_empty():
        logger.info(f"-no splice graph created for contig: {contig_acc}.... skipping.")
        # continue
        return

    if LRAA_Globals.DEBUG:
        sg.write_intron_exon_splice_graph_bed_files(
            "{}/__prereconstruct.{}.{}.pad1".format(
                prereconstruct_info_dir, contig_acc, contig_strand
            ),
            pad=1,
        )

    # Incorporate reference transcripts if provided
    lraa_obj = LRAA(sg, CPU)

    if input_transcripts:
        lraa_obj.assign_transcripts_paths_in_graph(input_transcripts)
        if LRAA_Globals.DEBUG:
            report_transcript_paths_in_graph(
                input_transcripts, "__input_transcripts_path_in_graph"
            )

    # build graph path nodes based on reads (and input transcripts) paths through graph.
    logger.info(f"\n// -building multpath graph for {contig_acc} {contig_strand}")
    mp_counter = lraa_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        allow_spacers,
        input_transcripts,
    )

    # Define isoforms
    logger.info(f"\n// -begin reconstructing isoforms for {contig_acc}")
    transcripts = lraa_obj.reconstruct_isoforms(single_best_only)

    if len(transcripts) == 0:
        logger.info(
            "no transcripts constructed on {} {}".format(contig_acc, contig_strand)
        )
        # continue
        return

    ###########################################################
    # Do an initial quant
    # Now use the original bam for quant (not depth normalized)

    logger.info(
        "\n//SECTION QUANT using all reads for assembled isoforms: Assigning reads to paths in graph."
    )

    mp_counter = lraa_obj.build_multipath_graph(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_quant,
        allow_spacers,
        input_transcripts,
    )

    q = Quantify(
        run_EM,
        LRAA_Globals.config["max_EM_iterations_during_asm"],
        quant_mode="draft",
    )
    frac_read_assignments = q.quantify(sg, transcripts, mp_counter)

    ##########################################
    ## Filtering of isoforms to remove 'noise'
    #########################################

    if input_transcripts is not None:
        lraa_obj.differentiate_known_vs_novel_isoforms(transcripts)

    if NO_FILTER_ISOFORMS:

        logger.info("NOT FILTERING ISOFORMS.")

    else:

        ##################################################################
        ## initial filter of novel isoforms with insufficient read support

        logger.info("\n// Filtering novel isoforms by min read support")
        min_read_support_novel_isoforms = LRAA_Globals.config["min_reads_novel_isoform"]
        transcripts = TranscriptFiltering.filter_novel_isoforms_by_min_read_support(
            transcripts, min_read_support_novel_isoforms
        )

        if len(transcripts) < 1:
            logger.info(
                "-no transcripts on contig {} {} survived min novel isoform read threshold requirement".format(
                    contig_acc, contig_strand
                )
            )
            # continue
            return

        # rerun quant for post-filtering
        logger.info(
            "\n// -rerunning quant post filtering novel isoforms via read thresholds"
        )
        frac_read_assignments = q.quantify(sg, transcripts, mp_counter)

        #####################################
        # pruning likely degradation products

        logger.info("\n// -pruning likely degradation products")

        transcripts = TranscriptFiltering.prune_likely_degradation_products(
            transcripts, sg, frac_read_assignments
        )

        if len(transcripts) < 1:
            logger.info(
                "-no transcripts on contig {} {} survived pruning degradation products".format(
                    contig_acc, contig_strand
                )
            )
            # continue
            return

        # rerun quant for post-filtering
        logger.info("\n// -rerunning quant post-filtering of degradation products")
        frac_read_assignments = q.quantify(sg, transcripts, mp_counter)

        #######################################################
        # Filter isoforms according to minimum isoform fraction

        logger.info("\n// -filtering isoforms according to minimum isoform fraction")
        if min_isoform_fraction > 0:
            transcripts = TranscriptFiltering.filter_isoforms_by_min_isoform_fraction(
                transcripts,
                min_isoform_fraction,
                run_EM,
                LRAA_Globals.config["max_EM_iterations_during_asm"],
            )

            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived isoform filtering".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

        ##################################################################
        # Filter monoexonic transcripts according to expression thresholds
        if LRAA_Globals.config["min_monoexonic_TPM"] > 0:
            logger.info("\n// -filtering monoexonic transcripts based on min TPM")
            transcripts = (
                TranscriptFiltering.filter_monoexonic_isoforms_by_TPM_threshold(
                    transcripts, LRAA_Globals.config["min_monoexonic_TPM"]
                )
            )
            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived monoexonic filtering strategy".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

        ####################################################################
        # Filter isoforms apparently derived from internal priming artifacts
        if LRAA_Globals.config["filter_internal_priming"]:
            logger.info("\n// -filtering out internal priming")
            transcripts = TranscriptFiltering.filter_internally_primed_transcripts(
                transcripts, contig_seq_str
            )
            if len(transcripts) < 1:
                logger.info(
                    "-no transcripts on contig {} {} survived internal priming filtering".format(
                        contig_acc, contig_strand
                    )
                )
                # continue
                return

    ###################
    ## Final quant step
    ###################

    LRAA_Globals.config["EM_alpha"] = quant_EM_alpha  # restore for final quant step.

    q = run_quant_only(
        contig_acc,
        contig_strand,
        contig_seq_str,
        bam_file_for_sg,
        bam_file_for_quant,
        restrict_region_lend,
        restrict_region_rend,
        transcripts,
        ofh_quant_output,
        ofh_quant_read_tracker,
        ofh_quant_read_tracking_lmdb,
        CPU,
        run_EM,
        prereconstruct_info_dir,
        report_quants=False,
    )

    ## one last filtering based on min isoform fraction

    transcripts_kept, transcripts_removed = (
        remove_low_isoform_fraction_transcripts_final_attempt(
            transcripts, min_isoform_fraction
        )
    )

    if len(transcripts_removed) > 0:
        transcripts = transcripts_kept

    frac_read_assignments = q._estimate_isoform_read_support(
        transcripts
    )  ##//TODO: make frac read assignments a member of the quantifier obj. Shouldn't need to run it again here unless transcripts got filtered above.

    ###############################################
    # examine splice compatible isoform differences (note, not currently filtering here, just annotating feature)
    (
        transcript_splice_compatible_containments,
        transcript_splice_compatible_contained_by,
    ) = TranscriptFiltering.evaluate_splice_compatible_alt_isoforms(transcripts)

    q.report_quant_results(
        transcripts,
        frac_read_assignments,
        ofh_quant_output,
        ofh_quant_read_tracker,
        splice_compatible_containments=transcript_splice_compatible_containments,
        splice_compatible_contained_by=transcript_splice_compatible_contained_by,
    )

    #######################################
    ## Done filtering, report final results

    ## report transcripts in GTF format
    logger.info(
        "writing gtf output for {} [{}] containing {} transcripts".format(
            contig_acc, contig_strand, len(transcripts)
        )
    )

    if LRAA_Globals.DEBUG:
        report_transcript_paths_in_graph(
            transcripts, "__output_transcripts_path_in_graph"
        )

    for transcript in transcripts:
        ofh_gtf.write(transcript.to_GTF_format() + "\n")

    return


def remove_low_isoform_fraction_transcripts_final_attempt(
    transcripts, min_isoform_fraction
):

    transcripts_kept = list()
    transcripts_removed = list()

    for transcript in transcripts:
        if transcript.get_isoform_fraction() < min_isoform_fraction:
            transcripts_removed.append(transcript)
        else:
            transcripts_kept.append(transcript)

    return transcripts_kept, transcripts_removed


def get_genome_contigs_listing(genome_fasta_filename):

    fai_file = "{}.fai".format(genome_fasta_filename)
    if not os.path.exists(fai_file):
        subprocess.check_call(
            "samtools faidx {}".format(genome_fasta_filename), shell=True
        )

    contigs_list = list()

    with open(fai_file) as fh:
        for line in fh:
            vals = line.split("\t")
            contig_acc = vals[0]
            contigs_list.append(contig_acc)

    return contigs_list


def report_transcript_paths_in_graph(transcripts, filename):

    with open(filename, "at") as ofh:
        for transcript in transcripts:
            sp = transcript.get_simple_path()
            exon_segments = transcript.get_exon_segments()
            transcript_id = transcript.get_transcript_id()
            print("\t".join([transcript_id, str(exon_segments), str(sp)]), file=ofh)

    return


def count_reads_from_bam(bam_filename):

    read_count_file = os.path.basename(bam_filename) + ".count"
    if not os.path.exists(read_count_file):
        subprocess.check_call(
            f"samtools view -c {bam_filename} > {read_count_file}.tmp", shell=True
        )
        os.rename(f"{read_count_file}.tmp", read_count_file)

    with open(read_count_file, "rt") as fh:
        count = next(fh)
        count = count.rstrip()
        count = int(count)
        assert count > 0, "Error, no reads counted from bam file..."
        return count


if __name__ == "__main__":
    main()
